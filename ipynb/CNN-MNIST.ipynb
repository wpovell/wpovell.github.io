{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from [siddk's tensorflow workshop](https://github.com/siddk/tensorflow-workshop)\n",
    "\n",
    "## The Math\n",
    "\n",
    "$\\begin{align*}\n",
    "X &= \\text{ Image Matrix } \\\\\n",
    "C_1 &= \\operatorname{ReLU}(\\operatorname{conv2d}(X, W_{c1}) + B_{c1}) \\\\\n",
    "P_1 &= \\operatorname{maxPool}(C_1) \\\\\n",
    "C_2 &= \\operatorname{ReLU}(\\operatorname{conv2d}(P_1, W_{c2}) + B_{c2}) \\\\\n",
    "P_2 &= \\operatorname{maxPool}(C_2) \\\\\n",
    "F &= \\text{ Flattened form of } P_2 \\\\\n",
    "H &= \\operatorname{ReLU}(F \\times W_{f1} + b_{f1}) \\\\\n",
    "O &= \\operatorname{Softmax}(H \\times W_{f2} + b_{f2}) \\\\\n",
    "L &= \\operatorname{Loss}(O, Y)\n",
    "\\end{align*}$\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf \n",
    "\n",
    "# Fetch MNIST Dataset using the supplied Tensorflow Utility Function\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Setup the Model Parameters\n",
    "INPUT_SIZE, OUTPUT_SIZE = 784, 10  \n",
    "FILTER_SIZE, FILTER_ONE_DEPTH, FILTER_TWO_DEPTH = 5, 32, 64\n",
    "FLAT_SIZE, HIDDEN_SIZE = 7 * 7 * 64, 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and Pooling\n",
    "\n",
    "The two main transformations used in the CNN different from the FFNN are convolution and pooling.\n",
    "\n",
    "### Convolution\n",
    "\n",
    "From which the CNN gets its name, Convolution is a process of applying filters to the image. In our case, the filter is a 5x5 matrix which is positioned over every cell of the image, multiplying pairwise and summing. For our model we have 32 of these filters in the first convolution, and 64 for the second. The values in these are trained by TensorFlow to minimize the loss.\n",
    "\n",
    "<img src=\"imgs/ipynb/conv.gif\" style=\"width:250px\"></img>\n",
    "[Source](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution)\n",
    "\n",
    "### Pooling\n",
    "\n",
    "Pooling shrinks the outputted feature maps from the convolution, removing less important information. In max pooling, each 2x2 area is condensed to its largest value, cutting each side length in half (28 to 14 and 14 to 7). Average pooling is also used where the average of each pool is outputted into the new cell.\n",
    "\n",
    "<img src=\"imgs/ipynb/maxPool.jpg\" style=\"width:500px\"></img>\n",
    "[Source](http://cs231n.github.io/convolutional-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Convolution/Pooling Helper Functions \n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "### Start Building the Computation Graph ###\n",
    "\n",
    "# Initializer - initialize our variables from standard normal with stddev 0.1\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "# Setup Placeholders => None argument in shape lets us pass in arbitrary sized batches\n",
    "X = tf.placeholder(tf.float32, shape=[None, INPUT_SIZE])  \n",
    "Y = tf.placeholder(tf.float32, shape=[None, OUTPUT_SIZE])\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "\n",
    "# Reshape input so it resembles an image (height x width x depth)\n",
    "X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "# Conv Filter 1 Variables\n",
    "Wconv_1 = tf.get_variable(\"WConv_1\", shape=[FILTER_SIZE, FILTER_SIZE,\n",
    "                                            1, FILTER_ONE_DEPTH], initializer=initializer)\n",
    "bconv_1 = tf.get_variable(\"bConv_1\", shape=[FILTER_ONE_DEPTH], initializer=initializer)\n",
    "\n",
    "# First Convolutional + Pooling Transformation\n",
    "h_conv1 = tf.nn.relu(conv2d(X_image, Wconv_1) + bconv_1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# Conv Filter 2 Variables\n",
    "Wconv_2 = tf.get_variable(\"WConv_2\", shape=[FILTER_SIZE, FILTER_SIZE,\n",
    "                                            FILTER_ONE_DEPTH, FILTER_TWO_DEPTH],\n",
    "                          initializer=initializer)\n",
    "bconv_2 = tf.get_variable(\"bConv_2\", shape=[FILTER_TWO_DEPTH], initializer=initializer)\n",
    "\n",
    "# Second Convolutional + Pooling Transformation\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, Wconv_2) + bconv_2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward & Dropout\n",
    "\n",
    "Our model finishes with flattening out last pool into a giant 3136 long vector and running it through our FFNN model from the last notebook. However, there is a small but important addition.\n",
    "\n",
    "Dropout disables nodes with a probability of `keep_prob`, removing them as well as their connections from the graph. This also means they aren't trained during that particular iteration. This limits the performance of the model, preventing it from over fitting the train dataset.\n",
    "\n",
    "<img src=\"imgs/ipynb/dropout.png\" style=\"width: 500px\"></img>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Flatten Convolved Image, into vector for remaining feed-forward transformations\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, FLAT_SIZE])\n",
    "\n",
    "# Hidden Layer Variables\n",
    "W_1 = tf.get_variable(\"W_1\", shape=[FLAT_SIZE, HIDDEN_SIZE], initializer=initializer)\n",
    "b_1 = tf.get_variable(\"b_1\", shape=[HIDDEN_SIZE], initializer=initializer)\n",
    "\n",
    "# Hidden Layer Transformation\n",
    "hidden = tf.nn.relu(tf.matmul(h_pool2_flat, W_1) + b_1)\n",
    "\n",
    "# DROPOUT - For regularization\n",
    "hidden_drop = tf.nn.dropout(hidden, keep_prob)\n",
    "\n",
    "# Output Layer Variables\n",
    "W_2 = tf.get_variable(\"W_2\", shape=[HIDDEN_SIZE, OUTPUT_SIZE], initializer=initializer)\n",
    "b_2 = tf.get_variable(\"b_2\", shape=[OUTPUT_SIZE], initializer=initializer)\n",
    "\n",
    "# Output Layer Transformation\n",
    "output = tf.matmul(hidden_drop, W_2) + b_2\n",
    "\n",
    "# Compute Loss\n",
    "loss = tf.losses.softmax_cross_entropy(Y, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Current Training Accuracy: 9.000\n",
      "Step 100 Current Training Accuracy: 91.000\n",
      "Step 200 Current Training Accuracy: 96.000\n",
      "Step 300 Current Training Accuracy: 99.000\n",
      "Step 400 Current Training Accuracy: 96.000\n",
      "Step 500 Current Training Accuracy: 95.000\n",
      "Step 600 Current Training Accuracy: 98.000\n",
      "Step 700 Current Training Accuracy: 97.000\n",
      "Step 800 Current Training Accuracy: 99.000\n",
      "Step 900 Current Training Accuracy: 99.000\n",
      "Test Accuracy: 98.730\n"
     ]
    }
   ],
   "source": [
    "# Compute Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1))\n",
    "accuracy = 100 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Setup Optimizer\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "### Launch the Session, to Communicate with Computation Graph ###\n",
    "BATCH_SIZE, NUM_TRAINING_STEPS = 100, 1000\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables in the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(NUM_TRAINING_STEPS):\n",
    "        batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "        curr_acc, _ = sess.run([accuracy, train_op], feed_dict={X: batch_x,\n",
    "                                                                Y: batch_y,\n",
    "                                                                keep_prob: 0.5})\n",
    "        if i % 100 == 0:\n",
    "            print('Step {} Current Training Accuracy: {:.3f}'.format(i, curr_acc))\n",
    "    \n",
    "    # Evaluate on Test Data\n",
    "    # keep-prop = 1.0 to disable dropout\n",
    "    print('Test Accuracy: {:.3f}'.format(sess.run(accuracy, feed_dict={X: mnist.test.images, \n",
    "                                                                Y: mnist.test.labels,\n",
    "                                                                keep_prob: 1.0}))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
