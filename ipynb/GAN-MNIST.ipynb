{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from [wiseodd on GitHub](https://github.com/wiseodd/generative-models/blob/master/GAN/vanilla_gan/gan_tensorflow.py).\n",
    "\n",
    "## GANs vs CGANs\n",
    "\n",
    "I'm going to show the implementation of a CGAN here, because they're almost the same as GANs but a little bit cooler. The only different is in our input. With a GAN, the generator takes in just a vector of noise. In a CGAN, the generator and the descriminator also take in a one-hot vector of the digit they're able to generate. The GAN is trying to learn the distribution of digits, so it makes sense that they'd learn better if we tell them the distinctions between digits, each of which are going to have their own distribution.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import os\n",
    "\n",
    "# Fetch MNIST Dataset using the supplied Tensorflow Utility Function\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Networks\n",
    "\n",
    "One new thing here is Xavier initialization, which isn't specific to GAN's but the original author of this code decided to use it. There's a great [blog post](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) about it that explains the statistics behind it, but the main idea is that it's a way of initializing variable weights such that values don't blow up or disappear as they flow through the network. In TensorFlow, it's implemented as pulling values from a uniform distribution $W$ with $$ \\operatorname{Var}(W) = \\frac{2}{n_\\text{in} + n_\\text{out}} $$ where $n_\\text{in}$ is the number of inputs into the neuron and $n_\\text{out}$ is the number out.\n",
    "\n",
    "### Generator\n",
    "\n",
    "Our generator for this implementation is just a simple feed-forward network taking in a $1 \\times 100$ vector of noise  concatted to a 1x10 one-hot vector, and transforming it into a $1 \\times 784$ MNIST image, with a $1 \\times 128$ ReLU hidden layer in the middle. The output of the last layer is run through a sigmoid to map the logits to a valid pixel intensity between 0 and 1. The exact formula is $$\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The size of the noise vector\n",
    "NOISE_SIZE = 100\n",
    "HIDDEN_SIZE = 128\n",
    "IMAGE_SIZE = 28*28\n",
    "N_DIGITS = 10\n",
    "\n",
    "# The input vector of noise\n",
    "Z = tf.placeholder(tf.float32, shape=[None, NOISE_SIZE])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, N_DIGITS])\n",
    "# 1st layer's weights and bias\n",
    "G_W1 = tf.get_variable('G_W1', shape=[NOISE_SIZE + N_DIGITS, HIDDEN_SIZE],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "G_b1 = tf.get_variable('G_b1', shape=[HIDDEN_SIZE],\n",
    "                       initializer=tf.zeros_initializer())\n",
    "\n",
    "# 2nd layer's weights and bias\n",
    "G_W2 = tf.get_variable('G_W2', shape=[HIDDEN_SIZE, IMAGE_SIZE],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "G_b2 = tf.get_variable('G_b2', shape=[IMAGE_SIZE],\n",
    "                       initializer=tf.zeros_initializer())\n",
    "\n",
    "# The trainable generator variables\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "def generator(z, y):\n",
    "    ''' The generator net.\n",
    "    '''\n",
    "    inputs = tf.concat(axis=1, values=[z, y])\n",
    "    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "\n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriminator\n",
    "\n",
    "Our Descrimiator will be pretty much the same, however it will be transforming a $1 \\times 784$ MNIST image concatted to a one-hot vector into a single scalar between 0 and 1 representing the probability that the input image is a \"real\" MNIST image, with the knowledge of what digit it's supposed to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The input image\n",
    "X = tf.placeholder(tf.float32, shape=[None, IMAGE_SIZE])\n",
    "\n",
    "# 1st layer's weights and bias\n",
    "D_W1 = tf.get_variable('D_W1', shape=[IMAGE_SIZE + N_DIGITS, HIDDEN_SIZE],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "D_b1 = tf.get_variable('D_b1', shape=[HIDDEN_SIZE],\n",
    "                       initializer=tf.zeros_initializer())\n",
    "\n",
    "# 2nd layer's weights and bias\n",
    "D_W2 = tf.get_variable('D_W2', shape=[HIDDEN_SIZE, 1],\n",
    "                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "D_b2 = tf.get_variable('D_b2', shape=[1],\n",
    "                       initializer=tf.zeros_initializer())\n",
    "\n",
    "# The trainable discriminator variables\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "def discriminator(x, y):\n",
    "    '''The discriminator net.\n",
    "    '''\n",
    "    inputs = tf.concat(axis=1, values=[x, y])\n",
    "    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "\n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss\n",
    "\n",
    "We will define two loss functions, one for our generator and one for our descriminator. \n",
    "\n",
    "The descriminator is trying to give the real MNIST images a probability as close to 1 as possible, while also giving the generated images a probability close to 0. We can construct our loss function by summing the cross-entropy for each goal's respective logits.\n",
    "\n",
    "The generator's goal is even simpler, it just wants to have the output for its generated images as close to 1 as possible. It's loss function can just be the cross-entropy between the output logits for the generated image and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image created by the generator\n",
    "G_sample = generator(Z, Y)\n",
    "\n",
    "# Descriminator's output for the real MNIST image\n",
    "D_real, D_logit_real = discriminator(X, Y)\n",
    "# Descriminator's output for the generated MNIST image\n",
    "D_fake, D_logit_fake = discriminator(G_sample, Y)\n",
    "\n",
    "# Descriminator wants high probability for the real image\n",
    "D_loss_real = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=D_logit_real,\n",
    "        labels=tf.ones_like(D_logit_real)))\n",
    "# Descriminator also wants low probability for the generated image\n",
    "D_loss_fake = tf.reduce_mean(\n",
    "    tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        logits=D_logit_fake,\n",
    "        labels=tf.zeros_like(D_logit_fake)))\n",
    "# We sum these to get our total descriminator loss\n",
    "D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "# Generator wants high probability for the generated image\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake,\n",
    "                                                                labels=tf.ones_like(D_logit_fake)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For training we just go through in batches, generating 128 images and getting 128 real MNIST images, running the two optimizers. Every 1000 iterations, the program saves an output of 16 generated images and reports the current loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    '''Returns a uniform sample of values between\n",
    "    -1 and 1 of size [m, n].\n",
    "    '''\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def plot(samples):\n",
    "    '''Plots a grid of 16 generated images.\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# The optimizer for each net\n",
    "D_optimizer = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_optimizer = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "image_input = np.identity(N_DIGITS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Image counter\n",
    "    i = 0\n",
    "    for it in range(1000000):\n",
    "        # Save out image of 16 generated digits\n",
    "        if it % 1000 == 0:\n",
    "            samples = sess.run(G_sample,\n",
    "                               feed_dict={\n",
    "                                   Z: sample_Z(N_DIGITS, NOISE_SIZE),\n",
    "                                   Y: image_input\n",
    "                               })\n",
    "            fig = plot(samples)\n",
    "            plt.show()\n",
    "\n",
    "        # Get a batch of real MNIST images\n",
    "        X_batch, Y_batch = mnist.train.next_batch(BATCH_SIZE)\n",
    "        \n",
    "        # Run our optimizers\n",
    "        _, D_loss_curr = sess.run([D_optimizer, D_loss],\n",
    "                                  feed_dict={\n",
    "                                      X: X_batch,\n",
    "                                      Z: sample_Z(BATCH_SIZE, NOISE_SIZE),\n",
    "                                      Y: Y_batch\n",
    "                                  })\n",
    "        _, G_loss_curr = sess.run([G_optimizer, G_loss],\n",
    "                                  feed_dict={\n",
    "                                      Z: sample_Z(BATCH_SIZE, NOISE_SIZE),\n",
    "                                      Y: Y_batch\n",
    "                                  })\n",
    "\n",
    "        # Report loss\n",
    "        if it % 1000 == 0:\n",
    "            print('Iter: {}'.format(it))\n",
    "            print('D loss: {:.4}'. format(D_loss_curr))\n",
    "            print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<img style=\"width: 50%; margin: auto;\" src=\"imgs/ipynb/GAN-loss.png\"></img>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-0.png'></img><figcaption>Iteration 0</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-1.png'></img><figcaption>Iteration 1</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-2.png'></img><figcaption>Iteration 2</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-3.png'></img><figcaption>Iteration 3</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-4.png'></img><figcaption>Iteration 4</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-5.png'></img><figcaption>Iteration 5</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-10.png'></img><figcaption>Iteration 10</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-50.png'></img><figcaption>Iteration 50</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-100.png'></img><figcaption>Iteration 100</figcaption></figure>\n",
    "<figure class='inb'><img src='imgs/ipynb/GAN-200.png'></img><figcaption>Iteration 200</figcaption></figure>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
