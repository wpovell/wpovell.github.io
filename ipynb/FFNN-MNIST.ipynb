{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from [siddk's tensorflow workshop](https://github.com/siddk/tensorflow-workshop).\n",
    "\n",
    "## The Math\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "X &= \\text{ Flattened image as a vector of length } 1 \\times 784 \\\\\n",
    "Y &= \\text{ Digit Label } \\\\\n",
    "W_1 &= \\text{ 1st matrix of weights, } B_1 = \\text{ 1st vector of bias } \\\\\n",
    "W_2 &= \\text{ 2nd matrix of weights, } B_2 = \\text{ 2nd vector of bias } \\\\\n",
    "H &= \\operatorname{ReLU}(X \\times W_1 + B_1) \\\\\n",
    "O &= \\operatorname{Softmax}(H \\times W_2 + B_2) \\\\\n",
    "L &= \\operatorname{Loss}(O, Y)\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "$X, Y, H, \\text{ and } O$ all have an extra dimension of batch that I'm ignoring here for simplicity.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf \n",
    "\n",
    "# Fetch MNIST Dataset using the supplied Tensorflow Utility Function\n",
    "mnist = input_data.read_data_sets(\"data/MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Setup the Model Parameters\n",
    "INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE = 784, 100, 10  \n",
    "\n",
    "### Start Building the Computation Graph ###\n",
    "\n",
    "# Initializer - initialize our variables from standard normal with stddev 0.1\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "# Setup Placeholders => None argument in shape lets us pass in arbitrary sized batches\n",
    "X = tf.placeholder(tf.float32, shape=[None, INPUT_SIZE])  \n",
    "Y = tf.placeholder(tf.float32, shape=[None, OUTPUT_SIZE])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer & ReLU\n",
    "\n",
    "Going from the input layer to the hidden layer we have\n",
    "\n",
    "$$\\underset{(n \\times 100)}{\\text{H}} = \\operatorname{ReLU}(\\underset{(n \\times 784)}{\\text{X}} \\times\n",
    "\\underset{(784 \\times 100)}{\\text{$W_1$}} +\n",
    "\\underset{(1 \\times 100)}{\\text{$B_1$}} )$$\n",
    "\n",
    "Here ReLU (Rectified Linear Unit) is defined as $\\operatorname{ReLU}(x) = \\max(0,x)$ which is applied to every value. This acts as a non-linear transformation on the values, allowing the model to go beyond making linear predictions. Other functions such as sigmoids are used but ReLU is fast and produces good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden Layer Variables\n",
    "W_1 = tf.get_variable(\"Hidden_W\", shape=[INPUT_SIZE, HIDDEN_SIZE], initializer=initializer)\n",
    "b_1 = tf.get_variable(\"Hidden_b\", shape=[HIDDEN_SIZE], initializer=initializer)\n",
    "\n",
    "# Hidden Layer Transformation\n",
    "hidden = tf.nn.relu(tf.matmul(X, W_1) + b_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer & Softmax\n",
    "\n",
    "Going from the hidden layer to the output layer we have\n",
    "\n",
    "$$ \\underset{(n \\times 10)}{\\text{O}} = \\operatorname{Softmax}(\\underset{(n \\times 100)}{\\text{H}} \\times\n",
    "\\underset{(100 \\times 10)}{\\text{$W_2$}} +\n",
    "\\underset{(1 \\times 10)}{\\text{$B_2$}}) $$\n",
    "\n",
    "Softmax is defined as $$\\operatorname{Softmax}(\\vec{x})_j = \\frac{e^{x_j}}{\\sum_{i=1}^{\\vert \\vec{x} \\vert} e^{x_i}}$$ \n",
    "\n",
    "Essentially softmax takes a vector of real numbers and turn it into a vector of values between 0 and 1 that sum to 1, forming a valid probability distribution. The loss for the model is then the cross-entropy between the correct label and this probability vector (if you're not familiar with cross-entropy check out [this blog post](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)). This causes the model to slowly train towards the maximum probability in the output vector being the correct label. During testing, the maximum probability of the output vector is considered the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output Layer Variables\n",
    "W_2 = tf.get_variable(\"Output_W\", shape=[100, 10], initializer=initializer)\n",
    "b_2 = tf.get_variable(\"Output_b\", shape=[10], initializer=initializer)\n",
    "\n",
    "# Output Layer Transformation\n",
    "output = tf.matmul(hidden, W_2) + b_2\n",
    "\n",
    "# Compute Loss\n",
    "loss = tf.losses.softmax_cross_entropy(Y, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Current Training Accuracy: 9.000\n",
      "Step 100 Current Training Accuracy: 87.000\n",
      "Step 200 Current Training Accuracy: 89.000\n",
      "Step 300 Current Training Accuracy: 94.000\n",
      "Step 400 Current Training Accuracy: 92.000\n",
      "Step 500 Current Training Accuracy: 94.000\n",
      "Step 600 Current Training Accuracy: 96.000\n",
      "Step 700 Current Training Accuracy: 96.000\n",
      "Step 800 Current Training Accuracy: 95.000\n",
      "Step 900 Current Training Accuracy: 94.000\n",
      "Test Accuracy: 95.450\n"
     ]
    }
   ],
   "source": [
    "# Compute Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(output, 1))\n",
    "accuracy = 100 * tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Setup Optimizer\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "### Launch the Session, to Communicate with Computation Graph ###\n",
    "BATCH_SIZE, NUM_TRAINING_STEPS = 100, 1000\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables in the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(NUM_TRAINING_STEPS):\n",
    "        batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "        curr_acc, _ = sess.run([accuracy, train_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "        if i % 100 == 0:\n",
    "            print('Step {} Current Training Accuracy: {:.3f}'.format(i, curr_acc))\n",
    "    \n",
    "    # Evaluate on Test Data\n",
    "    print('Test Accuracy: {:.3f}'.format(sess.run(accuracy, feed_dict={X: mnist.test.images, \n",
    "                                                                Y: mnist.test.labels})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
