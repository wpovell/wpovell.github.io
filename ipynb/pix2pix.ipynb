{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from [affinelayer's TensorFlow implementation](https://github.com/affinelayer/pix2pix-tensorflow). I would highly recommend checking out [his post](https://github.com/affinelayer/pix2pix-tensorflow) for a high-level overview of how the model works. This post is more meant to walk through the code line-by-line.\n",
    "\n",
    "## Load Data\n",
    "\n",
    "First lets load in our data. To start we're going to work with the facades dataset. The input image will be a labeled version of the second picture of a building facade, with different colors representing different features like windows, doors, etc. Our goal will be to produce the second image from the first.\n",
    "\n",
    "Unlike prior posts, we're going to use some of TensorFlow's utilities for loading in data. We get our list of files, put them into a queue, and have a `WholeFileReader` read and decode each. Although this is a bit more cumbersome than using placeholders, from what I've read it can lead to speed up training when working with large amounts of data. It also has the effect of making the loading in of data be a part of the model itself, which is interesting.\n",
    "\n",
    "\n",
    "<img style=\"display:block;margin:auto\" src=\"imgs/ipynb/pix2pix/input_example.jpg\">\n",
    "\n",
    "The format of these images is the first half is the target photo, and the second half is the annotated version. We'll preprocess the image to have pixel values between $[-1, 1]$ and then cut it in half, assigning the first part to our target and the second to our input (flipped since we're mapping labeled $\\rightarrow$ photo).\n",
    "\n",
    "One additional thing we'll do it crop down the images from SCALE_SIZE to CROP_SIZE at a random offset. This is just what the authors describe in the paper, however it makes sense as a form of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Size to scale to\n",
    "SCALE_SIZE = 286\n",
    "# Size to crop to after scaling\n",
    "CROP_SIZE = 256\n",
    "def process_image(image, seed):\n",
    "    \"\"\" Process image to be inputted to model.\n",
    "     Scales pixels to [-1, 1] and crops to CROP_SIZE from SCALE_SIZE randomly.\n",
    "     \"\"\"\n",
    "    # [0, 1] => [-1, 1]\n",
    "    image = image * 2 - 1\n",
    "    \n",
    "    # Scale down to SCALE_SIZE\n",
    "    image = tf.image.resize_images(image, [SCALE_SIZE, SCALE_SIZE], method=tf.image.ResizeMethod.AREA)\n",
    "    \n",
    "    # Choose random offset from corner and crop to CROP_SIZE from it\n",
    "    offset = tf.cast(tf.floor(tf.random_uniform([2], 0, SCALE_SIZE - CROP_SIZE + 1, seed=seed)), dtype=tf.int32)\n",
    "    image = tf.image.crop_to_bounding_box(image, offset[0], offset[1], CROP_SIZE, CROP_SIZE)\n",
    "    return image\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "def load_data(path):\n",
    "    \"\"\" Loads data from path. \"\"\"\n",
    "    # All jpgs\n",
    "    input_paths = glob.glob(os.path.join(path, '*.jpg'))\n",
    "\n",
    "    # Data pipeline\n",
    "    path_queue = tf.train.string_input_producer(input_paths, shuffle=True)\n",
    "    reader = tf.WholeFileReader()\n",
    "    paths, contents = reader.read(path_queue)\n",
    "    raw_image = tf.image.decode_jpeg(contents)\n",
    "    raw_image = tf.image.convert_image_dtype(raw_image, dtype=tf.float32)\n",
    "    raw_image.set_shape([None, None, 3])\n",
    "\n",
    "    # Split image into left and right side\n",
    "    width = tf.shape(raw_image)[1]\n",
    "    left, right = raw_image[:, :width // 2, :], raw_image[:, width // 2:, :]\n",
    "\n",
    "    # Process image.\n",
    "    # Use same seed for left & right so both get cropped the same\n",
    "    seed = random.randint(0, 2 ** 31 - 1)\n",
    "    left = process_image(left, seed)\n",
    "    right = process_image(right, seed)\n",
    "\n",
    "    # Assign sides to input/target\n",
    "    inputs, targets = right, left\n",
    "\n",
    "    # Create batches\n",
    "    paths, inputs, targets = tf.train.batch([paths, inputs, targets], batch_size=BATCH_SIZE)\n",
    "    steps_per_epoch = int(math.ceil(len(input_paths) / BATCH_SIZE))\n",
    "    return paths, inputs, targets, steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n",
    "\n",
    "Before we start building the model, there are a couple functions we'll be using that we should define.\n",
    "\n",
    "Note that I'm wrapping a lot of these calls with `tf.variable_scope`. What this does is creates a namespace around the variables we define. This can be important when we have multiple variables called \"filter\", for example, but want to keep them distinct.\n",
    "\n",
    "### Conv & Deconv\n",
    "\n",
    "If you're not familiar with Convolution, check out my post on [Convolutional Neural Nets](/posts/cnn-mnist.html). Deconvolution (or Transposed Convolution) was new to me. Normally when performing convolution, we're applying filters in such a way that we decrease the size of our image, effectively downsampling it. Deconvolution does the opposite, upsampling an image to output a larger tensor. There is a lot of debate as what to call this; deconvolution has a well defined meaning from signal processing as the inverse of convolution, which is not what this operation does. Because of this, many people opt to use the name Transposed Convolution instead, which is also what the TensorFlow API does. I'm just going to use `deconv` in code because it's easier to type.\n",
    "\n",
    "What deconvolution ends up looking like is just convolution but with more padding around/between pixels:\n",
    "\n",
    "<div style='text-align: center'>\n",
    "<figure style=\"display: inline-block;\">\n",
    "<img src=\"imgs/ipynb/pix2pix/conv.gif\">\n",
    "<figcaption>Convolution with Stride 2</figcaption>\n",
    "</figure>\n",
    "<figure style=\"display: inline-block;\">\n",
    "<img src=\"imgs/ipynb/pix2pix/deconv.gif\">\n",
    "<figcaption>Deconvolution with Stride 2</figcaption>\n",
    "</figure>\n",
    "<p>Animations from <a href=\"https://github.com/vdumoulin/conv_arithmetic\">here</a></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(batch_input, out_channels, stride):\n",
    "    \"\"\" Convolution. \"\"\"\n",
    "    with tf.variable_scope(\"conv\"):\n",
    "        in_channels = batch_input.get_shape()[3]\n",
    "        conv_filter = tf.get_variable(\"filter\", [4, 4, in_channels, out_channels], dtype=tf.float32,\n",
    "                                      initializer=tf.random_normal_initializer(0, 0.02))\n",
    "        # [batch, in_height, in_width, in_channels], [filter_width, filter_height, in_channels, out_channels]\n",
    "        #     => [batch, out_height, out_width, out_channels]\n",
    "        padded_input = tf.pad(batch_input, [[0, 0], [1, 1], [1, 1], [0, 0]], mode=\"CONSTANT\")\n",
    "        conv = tf.nn.conv2d(padded_input, conv_filter, [1, stride, stride, 1], padding=\"VALID\")\n",
    "        return conv\n",
    "\n",
    "def deconv(batch_input, out_channels):\n",
    "    ''' Transposed Convolution. '''\n",
    "    with tf.variable_scope(\"deconv\"):\n",
    "        batch, in_height, in_width, in_channels = [int(d) for d in batch_input.get_shape()]\n",
    "        deconv_filter = tf.get_variable(\"filter\", [4, 4, out_channels, in_channels], dtype=tf.float32,\n",
    "                                        initializer=tf.random_normal_initializer(0, 0.02))\n",
    "        # [batch, in_height, in_width, in_channels], [filter_width, filter_height, out_channels, in_channels]\n",
    "        #     => [batch, out_height, out_width, out_channels]\n",
    "        deconv = tf.nn.conv2d_transpose(batch_input,\n",
    "                                        deconv_filter,\n",
    "                                        [batch, in_height * 2, in_width * 2, out_channels],\n",
    "                                        [1, 2, 2, 1],\n",
    "                                        padding=\"SAME\")\n",
    "        return deconv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "Leaky ReLU is a variation of normal ReLU that helps prevent dead neurons. With ReLU, values less than 0 are set to 0, and have no gradient. This creates the problem where a neuron can start always outputting 0 for any input and once in that state, can't get out of it since there is no gradient to go up. Leaky ReLU tries to fix this by having values less than zero have a slight negative slope, equivalent to the following:\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{ReLU}(x) &= \\max(0,x) \\\\\n",
    "\\operatorname{LReLU}(x,a) &= \\frac{1+a}{2}x + \\frac{1-a}{2} \\vert x \\vert  \\\\\n",
    "&= \\begin{cases}\n",
    "    x,&  x \\geq 0\\\\\n",
    "    ax,& x < 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img style=\"margin: 5px; border: 1px solid black; display: inline-block; width: 30%\" src=\"imgs/ipynb/pix2pix/relu.png\"><img style=\"margin: 5px; border: 1px solid black; display: inline-block; width: 30%\" src=\"imgs/ipynb/pix2pix/lrelu.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, a):\n",
    "    ''' Leaky ReLU.\n",
    "        x is our tensor.\n",
    "        a is the magnitude of negative slope for x < 0.\n",
    "    '''\n",
    "    return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Batch Normalization is a cool general technique for improving training. All it does is normalize the input to a layer for mean and variance, while also including a trainable bias and scale parameter that allows the amount of normalization to be adjusted. From [the paper](https://arxiv.org/pdf/1502.03167.pdf) where it was introduced:\n",
    "\n",
    "<img style=\"width: 40%; display: block; margin: auto;\" src=\"imgs/ipynb/pix2pix/batchnorm.png\">\n",
    "\n",
    "If $\\gamma \\approx \\sigma$ and $\\beta \\approx \\mu$ then the transformation becomes an identity, allowing the NN to disable the behavior if not beneficial. In general, however, it allows for higher learning rates, reduces the need for dropout as it has a regularizing effect, and reduces dependence on initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(inp):\n",
    "    ''' Batch Normalization. '''\n",
    "    with tf.variable_scope(\"batchnorm\"):\n",
    "        channels = inp.get_shape()[3]\n",
    "        offset = tf.get_variable(\"offset\",\n",
    "                                 [channels],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "        scale = tf.get_variable(\"scale\",\n",
    "                                [channels],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.random_normal_initializer(1.0, 0.02))\n",
    "\n",
    "        mean, variance = tf.nn.moments(inp, axes=[0, 1, 2], keep_dims=False)\n",
    "        variance_epsilon = 1e-5\n",
    "        normalized = tf.nn.batch_normalization(inp, mean, variance,\n",
    "                                               offset, scale, variance_epsilon=variance_epsilon)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with that out of the way, lets get to actually building our model. Like all GAN architectures, our model will have a generator and discriminator. Let's start with the generator.\n",
    "\n",
    "## Generator\n",
    "\n",
    "<figure>\n",
    "<img style=\"width: 100%;\" src=\"imgs/ipynb/pix2pix/generator.png\">\n",
    "<img style=\"display:block;width:50%;margin:auto;\" src=\"imgs/ipynb/pix2pix/units.png\">\n",
    "<figcaption style='text-align:center'>Images from <a href=\"https://affinelayer.com/pix2pix/\">affinelayer</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "The generator is made up of a series of convolution layers, followed by a series of deconvolution layers. This has an effect similar to an autoencoder, where the model is forced to compress the 256x256x3 image into a single 1x1x256 vector keeping only the most important parts of the image before upsampling it again with the deconvolution layers. However, often with these image-to-image translation tasks larger structural features are the same in both input and target, so the model has skip layers where layers from the convolution are inputted into their opposite deconvolution layer, allowing this information to transfer.\n",
    "\n",
    "Each layer in the encoder gets leaky ReLU and and decoder layers get normal ReLU and dropout. Both get batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Num of generator filters\n",
    "NGF = 64\n",
    "def create_generator(generator_inputs):\n",
    "    \"\"\" Create generator network. \"\"\"\n",
    "    layers = []\n",
    "\n",
    "    ### ENCODER ###\n",
    "\n",
    "    # encoder 1: [batch, 256, 256, 3] => [batch, 128, 128, NGF]\n",
    "    with tf.variable_scope(\"encoder_1\"):\n",
    "        output = conv(generator_inputs, NGF, stride=2)\n",
    "        layers.append(output)\n",
    "\n",
    "    layer_specs = [\n",
    "        NGF * 2,  # encoder 2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n",
    "        NGF * 4,  # encoder 3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n",
    "        NGF * 8,  # encoder 4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n",
    "        NGF * 8,  # encoder 5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n",
    "        NGF * 8,  # encoder 6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n",
    "        NGF * 8,  # encoder 7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n",
    "        NGF * 8,  # encoder 8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n",
    "    ]\n",
    "\n",
    "    for out_channels in layer_specs:\n",
    "        with tf.variable_scope(\"encoder_{}\".format(len(layers) + 1)):\n",
    "            rectified = lrelu(layers[-1], 0.2)\n",
    "            convolved = conv(rectified, out_channels, stride=2)\n",
    "            output = batchnorm(convolved)\n",
    "            layers.append(output)\n",
    "\n",
    "    ### DECODER ###\n",
    "\n",
    "    # (channels, dropout probability)\n",
    "    layer_specs = [\n",
    "        (NGF * 8, 0.5),  # decoder 8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.5),  # decoder 7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.5),  # decoder 6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.0),  # decoder 5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\n",
    "        (NGF * 4, 0.0),  # decoder 4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n",
    "        (NGF * 2, 0.0),  # decoder 3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n",
    "        (NGF, 0.0),  # decoder 2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n",
    "    ]\n",
    "\n",
    "    num_encoder_layers = len(layers)\n",
    "    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n",
    "        # Encoder layer to concat with to create skip connection\n",
    "        skip_layer = num_encoder_layers - decoder_layer - 1\n",
    "        with tf.variable_scope(\"decoder_{}\".format(skip_layer + 1)):\n",
    "            if decoder_layer == 0:\n",
    "                # First decoder layer doesn't have skip connections\n",
    "                inp = layers[-1]\n",
    "            else:\n",
    "                inp = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n",
    "\n",
    "            rectified = tf.nn.relu(inp)\n",
    "            output = deconv(rectified, out_channels)\n",
    "            output = batchnorm(output)\n",
    "\n",
    "            if dropout > 0.0:\n",
    "                output = tf.nn.dropout(output, keep_prob=1 - dropout)\n",
    "\n",
    "            layers.append(output)\n",
    "\n",
    "    # decoder 1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, 3]\n",
    "    with tf.variable_scope(\"decoder_1\"):\n",
    "        inp = tf.concat([layers[-1], layers[0]], axis=3)\n",
    "        rectified = tf.nn.relu(inp)\n",
    "        output = deconv(rectified, 3)\n",
    "        output = tf.tanh(output)\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "<figure>\n",
    "<img style=\"width: 100%\" src=\"imgs/ipynb/pix2pix/discriminator.png\">\n",
    "<figcaption style='text-align:center'>Image from <a href=\"https://affinelayer.com/pix2pix/\">affinelayer</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Next up is our discriminator, which is a lot simpler than the generator. The discriminator receives an input (a labeled facade in this case) and a generated or real output for that input that it has to gauge the legitimacy of. It is made up of a series of convolutions outputting a single probability of being \"real\" at the end. Each layer also gets batch normalization and leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of discriminator filters\n",
    "NDF = 64\n",
    "def create_discriminator(discrim_inputs, discrim_targets):\n",
    "    n_layers = 3\n",
    "    layers = []\n",
    "\n",
    "    # 2x [batch, height, width, 3] => [batch, height, width, 6]\n",
    "    inp = tf.concat([discrim_inputs, discrim_targets], axis=3)\n",
    "\n",
    "    # layer 1: [batch, 256, 256, 6] => [batch, 128, 128, ndf]\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        convolved = conv(inp, NDF, stride=2)\n",
    "        rectified = lrelu(convolved, 0.2)\n",
    "        layers.append(rectified)\n",
    "\n",
    "    # layer 2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n",
    "    # layer 3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n",
    "    # layer 4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n",
    "    for i in range(n_layers):\n",
    "        with tf.variable_scope(\"layer_{}\".format(len(layers) + 1)):\n",
    "            out_channels = NDF * min(2 ** (i + 1), 8)\n",
    "            stride = 1 if i == n_layers - 1 else 2  # Last layer here has stride 1\n",
    "            convolved = conv(layers[-1], out_channels, stride=stride)\n",
    "            normalized = batchnorm(convolved)\n",
    "            rectified = lrelu(normalized, 0.2)\n",
    "            layers.append(rectified)\n",
    "\n",
    "    # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\n",
    "    with tf.variable_scope(\"layer_{}\".format(len(layers) + 1)):\n",
    "        convolved = conv(rectified, out_channels=1, stride=1)\n",
    "        output = tf.sigmoid(convolved)\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Optimizer\n",
    "\n",
    "The discriminator's loss is simple, just minimize giving the wrong label.\n",
    "\n",
    "For the generator, we have two losses. The first is the standard GAN loss, try to minimize the discriminator detecting that the generated image is fake. The second loss is an L1 loss that tries to minimize the differences between generated and target image, since they should be as similar as possible. L2 loss is also sometimes used here, but the paper notes L1 loss produces less blurring. We then linearly combine these two losses to get our total generator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Val used to prevent 0 erros in log\n",
    "EPS = 1e-12\n",
    "# Weights for the two GAN losses\n",
    "GAN_WEIGHT = 1.\n",
    "L1_WEIGHT = 100.\n",
    "# Optimizer params\n",
    "LR = 0.0002\n",
    "BETA1 = 0.5\n",
    "def build_model(inputs, targets):\n",
    "    with tf.variable_scope(\"generator\"):\n",
    "        outputs = create_generator(inputs)\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\"):\n",
    "        predict_real = create_discriminator(inputs, targets)\n",
    "\n",
    "    with tf.variable_scope(\"discriminator\", reuse=True):\n",
    "        predict_fake = create_discriminator(inputs, outputs)\n",
    "\n",
    "    # Discriminator loss\n",
    "    discrim_loss = tf.reduce_mean(-(tf.log(predict_real + EPS) + tf.log(1 - predict_fake + EPS)))\n",
    "\n",
    "    # Generator loss\n",
    "    gen_loss_gan = tf.reduce_mean(-tf.log(predict_fake + EPS))\n",
    "    gen_loss_l1 = tf.reduce_mean(tf.abs(targets - outputs))\n",
    "    gen_loss = gen_loss_gan * GAN_WEIGHT + gen_loss_l1 * L1_WEIGHT\n",
    "\n",
    "    # Discriminator train op\n",
    "    discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"discriminator\")]\n",
    "    discrim_optim = tf.train.AdamOptimizer(LR, BETA1)\n",
    "    discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)\n",
    "    discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)\n",
    "\n",
    "    # Generator train op\n",
    "    with tf.control_dependencies([discrim_train]):\n",
    "        gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"generator\")]\n",
    "        gen_optim = tf.train.AdamOptimizer(LR, BETA1)\n",
    "        gen_grads_and_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_tvars)\n",
    "        gen_train = gen_optim.apply_gradients(gen_grads_and_vars)\n",
    "\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "    update_losses = ema.apply([discrim_loss, gen_loss_gan, gen_loss_l1])\n",
    "\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    incr_global_step = tf.assign(global_step, global_step + 1)\n",
    "\n",
    "    train_op = tf.group(update_losses, incr_global_step, gen_train)\n",
    "    return outputs, train_op, gen_loss, discrim_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Training is pretty straightforward, however, one thing to note here is that unlike prior models I've written up this definitely requires a GPU if you're going to do the full 200 epochs, or you'll be waiting a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "MAX_EPOCHS = 200\n",
    "# How often to print progress/save\n",
    "PROG_F = 50\n",
    "SAVE_F = 5000\n",
    "\n",
    "OUT_DIR = 'out'\n",
    "def train(train_op, outputs, gen_loss, discrim_loss, steps_per_epoch):\n",
    "    \"\"\" Train model. \"\"\"\n",
    "    max_steps = MAX_EPOCHS * steps_per_epoch\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    sv = tf.train.Supervisor(logdir=OUT_DIR, save_summaries_secs=0, saver=None)\n",
    "    with sv.managed_session() as sess:\n",
    "        start = time.time()\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            def should(freq):\n",
    "                return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)\n",
    "\n",
    "            fetches = {\n",
    "                \"train\": train_op,\n",
    "                \"global_step\": sv.global_step,\n",
    "            }\n",
    "\n",
    "            if should(PROG_F):\n",
    "                fetches[\"discrim_loss\"] = discrim_loss\n",
    "                fetches[\"gen_loss\"] = gen_loss\n",
    "\n",
    "            results = sess.run(fetches)\n",
    "\n",
    "            if should(PROG_F):\n",
    "                train_epoch = math.ceil(results[\"global_step\"] / steps_per_epoch)\n",
    "                train_step = (results[\"global_step\"] - 1) % steps_per_epoch + 1\n",
    "                rate = (step + 1) * BATCH_SIZE / (time.time() - start)\n",
    "                remaining = (max_steps - step) * BATCH_SIZE / rate\n",
    "                print((\"progress  \" +\n",
    "                       \"epoch {}  \" +\n",
    "                       \"step {}  \" +\n",
    "                       \"image/sec {:0.1f} \" +\n",
    "                       \"remaining {}m\").format(train_epoch, train_step, rate, int(remaining / 60)))\n",
    "\n",
    "                print(\"discrim_loss\", results[\"discrim_loss\"])\n",
    "                print(\"gen_loss_GAN\", results[\"gen_loss\"])\n",
    "\n",
    "            if should(SAVE_F):\n",
    "                print(\"saving model\")\n",
    "                saver.save(sess, os.path.join(OUT_DIR, \"model\"), global_step=sv.global_step)\n",
    "\n",
    "            if sv.should_stop():\n",
    "                break\n",
    "        saver.save(sess, os.path.join(OUT_DIR, \"model\"), global_step=sv.global_step)\n",
    "\n",
    "paths, inputs, targets, steps_per_epoch = load_data('data/pix2pix/facades/train')\n",
    "outputs, train_op, gen_loss, discrim_loss = build_model(inputs, targets)\n",
    "train(train_op, outputs, gen_loss, discrim_loss, steps_per_epoch)\n",
    "OUT_DIR = 'out'\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating\n",
    "\n",
    "Because of the way we're feeding in data, evaluating is a little different. We'll clear out everything, remake our model with the `val` data as input instead, and load in our trained parameters. From there, running is exactly the same except now we write out the images produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = 'out'\n",
    "def save_images(results):\n",
    "    \"\"\" Save images from results. \"\"\"\n",
    "    image_dir = os.path.join(OUT_DIR, \"images\")\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "    for i, in_path in enumerate(results[\"paths\"]):\n",
    "        num, _ = os.path.splitext(os.path.basename(in_path.decode('utf8')))\n",
    "        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n",
    "            filename = num + \"-\" + kind + \".png\"\n",
    "            out_path = os.path.join(image_dir, filename)\n",
    "            contents = results[kind][i]\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                f.write(contents)\n",
    "\n",
    "def deprocess_image(img):\n",
    "    ''' Turn output pixel values back into a normal image. '''\n",
    "    img = (img + 1) / 2\n",
    "    return tf.image.convert_image_dtype(img, dtype=tf.uint8, saturate=True)\n",
    "                \n",
    "def test(paths, inputs, targets, outputs, steps_per_epoch):\n",
    "    \"\"\" Save model output for images from data. \"\"\"\n",
    "    output_images = {\n",
    "        \"paths\": paths,\n",
    "        \"inputs\": tf.map_fn(tf.image.encode_png, deprocess_image(inputs),\n",
    "                            dtype=tf.string, name=\"inputs_pngs\"),\n",
    "        \"targets\": tf.map_fn(tf.image.encode_png, deprocess_image(targets),\n",
    "                            dtype=tf.string, name=\"target_pngs\"),\n",
    "        \"outputs\": tf.map_fn(tf.image.encode_png, deprocess_image(outputs),\n",
    "                            dtype=tf.string, name=\"output_pngs\"),\n",
    "    }\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    sv = tf.train.Supervisor(logdir=OUT_DIR, save_summaries_secs=0, saver=None)\n",
    "    with sv.managed_session() as sess:\n",
    "        # Restore from checkpoint\n",
    "        checkpoint = tf.train.latest_checkpoint(OUT_DIR)\n",
    "        saver.restore(sess, checkpoint)\n",
    "        # Save outputs\n",
    "        for step in range(steps_per_epoch):\n",
    "            results = sess.run(output_images)\n",
    "            save_images(results)\n",
    "\n",
    "paths, inputs, targets, steps_per_epoch = load_data('data/pix2pix/facades/val')\n",
    "outputs, _, _, _ = build_model(inputs, targets)\n",
    "test(paths, inputs, targets, outputs, steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "| Input | Generated | Target |\n",
    "|-------|-----------|--------|\n",
    "| <img src='imgs/ipynb/pix2pix/results/1-inputs.png'> | <img src='imgs/ipynb/pix2pix/results/1-outputs.png'> | <img src='imgs/ipynb/pix2pix/results/1-targets.png'> |\n",
    "| <img src='imgs/ipynb/pix2pix/results/2-inputs.png'> | <img src='imgs/ipynb/pix2pix/results/2-outputs.png'> | <img src='imgs/ipynb/pix2pix/results/2-targets.png'> |\n",
    "| <img src='imgs/ipynb/pix2pix/results/3-inputs.png'> | <img src='imgs/ipynb/pix2pix/results/3-outputs.png'> | <img src='imgs/ipynb/pix2pix/results/3-targets.png'> |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
