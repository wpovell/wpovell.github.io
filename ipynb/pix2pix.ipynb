{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from [affinelayer's TensorFlow implementation](https://github.com/affinelayer/pix2pix-tensorflow). I would highly recommend checking out [his post](https://github.com/affinelayer/pix2pix-tensorflow) for a high-level overview of how the model works. This post is more meant to walk through the code line-by-line.\n",
    "\n",
    "## Load Data\n",
    "\n",
    "First lets load in our data. To start we're going to work with the facades dataset. The input image will be a labeled version of the second picture of a building facade, with different colors representing different features like windows, doors, etc. Our goal will be to produce the second image from the first.\n",
    "\n",
    "Unlike prior posts, we're going to use some of TensorFlow's utilities for loading in data. We get our list of files, put them into a queue, and have a `WholeFileReader` read and decode each. The format of these images is the first half is the target photo, and the second half is the annotated version.\n",
    "\n",
    "<img style=\"display:block;margin:auto\" src=\"imgs/ipynb/pix2pix/input_example.jpg\">\n",
    "\n",
    "We'll preprocess the image to have pixel values between $[-1, 1]$ and then cut it in half, assigning the first part to our target and the second to our input (flipped since we're mapping labeled $\\rightarrow$ photo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "def loadData(path, shuffle=True):\n",
    "    ''' Loads in image data from path. '''\n",
    "    input_paths = glob.glob(os.path.join(path, '*.jpg')) # All jpgs\n",
    "    path_queue = tf.train.string_input_producer(input_paths, shuffle=shuffle) # Produces image paths\n",
    "    reader = tf.WholeFileReader()\n",
    "    paths, contents = reader.read(path_queue)\n",
    "    rawInput = tf.image.decode_jpeg(contents)\n",
    "    rawInput = tf.image.convert_image_dtype(rawInput, dtype=tf.float32)\n",
    "\n",
    "    # [height, width, channel]\n",
    "    rawInput.set_shape([None, None, 3])\n",
    "    width = tf.shape(rawInput)[1]\n",
    "\n",
    "    def process(r):\n",
    "        # Resize to 256x256\n",
    "        r = tf.image.resize_images(r, [256, 256], method=tf.image.ResizeMethod.AREA)\n",
    "        # Pix vals from [0, 1] => [-1, 1]\n",
    "        return r * 2 - 1\n",
    "\n",
    "    targets = process(rawInput[:,:width//2,:]) # Left side\n",
    "    inputs = process(rawInput[:,width//2:,:]) # Right side\n",
    "\n",
    "    paths, inputs, targets = tf.train.batch([paths, inputs, targets], batch_size=BATCH_SIZE)\n",
    "    steps_per_epoch = int(math.ceil(len(input_paths) / BATCH_SIZE))\n",
    "    return paths, inputs, targets, steps_per_epoch\n",
    "\n",
    "# Train data\n",
    "paths, inputs, targets, steps_per_epoch = loadData('data/pix2pix/facades/train')\n",
    "# Test data\n",
    "tst_paths, tst_inputs, tst_targets, tst_steps_per_epoch = loadData('data/pix2pix/facades/val', shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n",
    "\n",
    "Before we start building the model, there are a couple functions we'll be using that we should define.\n",
    "\n",
    "### Conv & Deconv\n",
    "\n",
    "If you're not familiar with Convolution, check out my post on [Convolutional Neural Nets](/posts/cnn-mnist.html). Deconvolution (or Transposed Convolution) was new to me. Normally when performing convolution, we're applying filters in such a way that we decrease the size of our image, effectively downsampling it. Deconvolution does the opposite, upsampling an image to output a larger tensor. There is a lot of debate as what to call this; deconvolution has a well defined meaning from signal processing as the inverse of convolution, which is not what this operation does. Because of this, many people opt to use the name Transposed Convolution instead, which is also what the TensorFlow API does. I'm just going to use `deconv` in code because it's easier to type.\n",
    "\n",
    "What deconvolution ends up looking like is just convolution but with more padding around/between pixels:\n",
    "\n",
    "<div style='text-align: center'>\n",
    "<figure style=\"display: inline-block;\">\n",
    "<img src=\"imgs/ipynb/pix2pix/conv.gif\">\n",
    "<figcaption>Convolution with Stride 2</figcaption>\n",
    "</figure>\n",
    "<figure style=\"display: inline-block;\">\n",
    "<img src=\"imgs/ipynb/pix2pix/deconv.gif\">\n",
    "<figcaption>Deconvolution with Stride 2</figcaption>\n",
    "</figure>\n",
    "<p>Animations from <a href=\"https://github.com/vdumoulin/conv_arithmetic\">here</a></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(batch_input, out_channels, stride):\n",
    "    ''' Convolve input with given stride. '''\n",
    "    with tf.variable_scope(\"conv\"):\n",
    "        in_channels = batch_input.get_shape()[3]\n",
    "        # The trainable filter we create for the conv\n",
    "        conv_filter = tf.get_variable(\"filter\",\n",
    "                                      [4, 4, in_channels, out_channels],\n",
    "                                      dtype=tf.float32,\n",
    "                                      initializer=tf.random_normal_initializer(0, 0.02))\n",
    "\n",
    "        padded_input = tf.pad(batch_input,\n",
    "                              [[0, 0], [1, 1], [1, 1], [0, 0]],\n",
    "                              mode=\"CONSTANT\")\n",
    "        # Output of the conv\n",
    "        conv = tf.nn.conv2d(padded_input,\n",
    "                            conv_filter,\n",
    "                            [1, stride, stride, 1],\n",
    "                            padding=\"VALID\")\n",
    "        return conv\n",
    "\n",
    "def deconv(batch_input, out_channels):\n",
    "    ''' Transposed Convolution. '''\n",
    "    with tf.variable_scope(\"deconv\"):\n",
    "        batch, in_height, in_width, in_channels = [int(d) for d in batch_input.get_shape()]\n",
    "\n",
    "        # The trainable filter we create for the deconv\n",
    "        conv_filter = tf.get_variable(\"filter\",\n",
    "                                      [4, 4, out_channels, in_channels],\n",
    "                                      dtype=tf.float32,\n",
    "                                      initializer=tf.random_normal_initializer(0, 0.02))\n",
    "        # Output of the deconv\n",
    "        conv = tf.nn.conv2d_transpose(batch_input,\n",
    "                                      conv_filter,\n",
    "                                      [batch, in_height * 2, in_width * 2, out_channels],\n",
    "                                      [1, 2, 2, 1],\n",
    "                                      padding=\"SAME\")\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "Leaky ReLU is a variation of normal ReLU that helps prevent dead neurons. With ReLU, values less than 0 are set to 0, and have no gradient. This creates the problem where a neuron can start always outputting 0 for any input and once in that state, can't get out of it since there is no gradient to go up. Leaky ReLU tries to fix this by having values less than zero have a slight negative slope, equivalent to the following:\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{ReLU}(x) &= \\max(0,x) \\\\\n",
    "\\operatorname{LReLU}(x,a) &= \\frac{1+a}{2}x + \\frac{1-a}{2} \\vert x \\vert  \\\\\n",
    "&= \\begin{cases}\n",
    "    x,&  x \\geq 0\\\\\n",
    "    ax,& x < 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img style=\"margin: 5px; border: 1px solid black; display: inline-block; width: 30%\" src=\"imgs/ipynb/pix2pix/relu.png\"><img style=\"margin: 5px; border: 1px solid black; display: inline-block; width: 30%\" src=\"imgs/ipynb/pix2pix/lrelu.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, a):\n",
    "    ''' Leaky ReLU.\n",
    "        x is our tensor.\n",
    "        a is the magnitude of negative slope for x < 0.\n",
    "    '''\n",
    "    return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Batch Normalization is a cool general technique for improving training. All it does is normalize the input to a layer for mean and variance, while also including a trainable bias and scale parameter that allows the amount of normalization to be adjusted. From [the paper](https://arxiv.org/pdf/1502.03167.pdf) where it was introduced:\n",
    "\n",
    "<img style=\"width: 40%; display: block; margin: auto;\" src=\"imgs/ipynb/pix2pix/batchnorm.png\">\n",
    "\n",
    "If $\\gamma \\approx \\sigma$ and $\\beta \\approx \\mu$ then the transformation becomes an identity, allowing the NN to disable the behavior if not beneficial. In general, however, it allows for higher learning rates, reduces the need for dropout as it has a regularizing effect, and reduces dependence on initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(inp):\n",
    "    ''' Batch Normalization. '''\n",
    "    with tf.variable_scope(\"batchnorm\"):\n",
    "        channels = inp.get_shape()[3]\n",
    "        offset = tf.get_variable(\"offset\",\n",
    "                                 [channels],\n",
    "                                 dtype=tf.float32,\n",
    "                                 initializer=tf.zeros_initializer())\n",
    "        scale = tf.get_variable(\"scale\",\n",
    "                                [channels],\n",
    "                                dtype=tf.float32,\n",
    "                                initializer=tf.random_normal_initializer(1.0, 0.02))\n",
    "\n",
    "        mean, variance = tf.nn.moments(inp, axes=[0, 1, 2], keep_dims=False)\n",
    "        variance_epsilon = 1e-5\n",
    "        normalized = tf.nn.batch_normalization(inp, mean, variance,\n",
    "                                               offset, scale, variance_epsilon=variance_epsilon)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with that out of the way, lets get to actually building our model. Like all GAN architectures, our model will have a generator and discriminator. Let's start with the generator.\n",
    "\n",
    "## Generator\n",
    "\n",
    "<figure>\n",
    "<img style=\"width: 100%;\" src=\"imgs/ipynb/pix2pix/generator.png\">\n",
    "<img style=\"display:block;width:50%;margin:auto;\" src=\"imgs/ipynb/pix2pix/units.png\">\n",
    "<figcaption style='text-align:center'>Images from <a href=\"https://affinelayer.com/pix2pix/\">affinelayer</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "The generator is made up of a series of convolution layers, followed by a series of deconvolution layers. This has an effect similar to an autoencoder, where the model is forced to compress the 256x256x3 image into a single 1x1x256 vector keeping only the most important parts of the image before upsampling it again with the deconvolution layers. However, often with these image-to-image translation tasks larger structural features are the same in both input and target, so the model has skip layers where layers from the convolution are inputted into their opposite deconvolution layer, allowing this information to transfer.\n",
    "\n",
    "Each layer in the encoder gets leaky ReLU and and decoder layers get normal ReLU and dropout. Both get batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of generator filters\n",
    "NGF = 64\n",
    "\n",
    "def create_generator(generator_inputs):\n",
    "    ''' Creates our generator for the given inputs. '''\n",
    "    layers = []\n",
    "\n",
    "    # encoder_1: [batch, 256, 256, 3] => [batch, 128, 128, ngf]\n",
    "    # This layer doesn't get batchnorm (from paper)\n",
    "    with tf.variable_scope(\"encoder_1\"):\n",
    "        output = conv(generator_inputs, NGF, stride=2)\n",
    "        layers.append(output)\n",
    "\n",
    "    layer_specs = [\n",
    "        NGF * 2, # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n",
    "        NGF * 4, # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n",
    "        NGF * 8, # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n",
    "        NGF * 8, # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n",
    "        NGF * 8, # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n",
    "        NGF * 8, # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n",
    "        NGF * 8, # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n",
    "    ]\n",
    "\n",
    "    for out_channels in layer_specs:\n",
    "        with tf.variable_scope(\"encoder_%d\" % (len(layers) + 1)):\n",
    "            rectified = lrelu(layers[-1], 0.2)\n",
    "            # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\n",
    "            convolved = conv(rectified, out_channels, stride=2)\n",
    "            output = batchnorm(convolved)\n",
    "            layers.append(output)\n",
    "\n",
    "    layer_specs = [\n",
    "        (NGF * 8, 0.5),   # decoder_8: [batch, 1, 1, ngf * 8]       => [batch, 2, 2, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.5),   # decoder_7: [batch, 2, 2, ngf * 8 * 2]   => [batch, 4, 4, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.5),   # decoder_6: [batch, 4, 4, ngf * 8 * 2]   => [batch, 8, 8, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.0),   # decoder_5: [batch, 8, 8, ngf * 8 * 2]   => [batch, 16, 16, ngf * 8 * 2]\n",
    "        (NGF * 4, 0.0),   # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n",
    "        (NGF * 2, 0.0),   # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n",
    "        (NGF, 0.0),       # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n",
    "    ]\n",
    "\n",
    "    num_encoder_layers = len(layers)\n",
    "    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n",
    "        # Conv layer to connect to\n",
    "        skip_layer = num_encoder_layers - decoder_layer - 1\n",
    "        with tf.variable_scope(\"decoder_%d\" % (skip_layer + 1)):\n",
    "            if decoder_layer == 0:\n",
    "                # First decoder layer doesn't have skip connections\n",
    "                # since it is directly connected to the skip_layer.\n",
    "                inp = layers[-1]\n",
    "            else:\n",
    "                inp = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n",
    "\n",
    "            rectified = tf.nn.relu(inp)\n",
    "            # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\n",
    "            output = deconv(rectified, out_channels)\n",
    "            output = batchnorm(output)\n",
    "\n",
    "            if dropout > 0.0:\n",
    "                output = tf.nn.dropout(output, keep_prob=1 - dropout)\n",
    "\n",
    "            layers.append(output)\n",
    "\n",
    "    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, 3]\n",
    "    with tf.variable_scope(\"decoder_1\"):\n",
    "        inp = tf.concat([layers[-1], layers[0]], axis=3)\n",
    "        rectified = tf.nn.relu(inp)\n",
    "        output = deconv(rectified, 3)\n",
    "        output = tf.tanh(output) # Limits output to (-1, 1)\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "<figure>\n",
    "<img style=\"width: 100%\" src=\"imgs/ipynb/pix2pix/discriminator.png\">\n",
    "<figcaption style='text-align:center'>Image from <a href=\"https://affinelayer.com/pix2pix/\">affinelayer</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Next up is our discriminator, which is a lot simpler than the generator. The discriminator receives an input (a labeled facade in this case) and a generated or real output for that input that it has to gauge the legitimacy of. It is made up of a series of convolutions outputting a single probability of being \"real\" at the end. Each layer also gets batch normalization and leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of discriminator filters\n",
    "NDF = 64\n",
    "\n",
    "def create_discriminator(discrim_inputs, discrim_targets):\n",
    "    n_layers = 3\n",
    "    layers = []\n",
    "\n",
    "    # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]\n",
    "    inp = tf.concat([discrim_inputs, discrim_targets], axis=3)\n",
    "\n",
    "    # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]\n",
    "    with tf.variable_scope(\"layer_1\"):\n",
    "        convolved = conv(inp, NDF, stride=2)\n",
    "        rectified = lrelu(convolved, 0.2)\n",
    "        layers.append(rectified)\n",
    "\n",
    "    # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n",
    "    # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n",
    "    # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n",
    "    for i in range(n_layers):\n",
    "        with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n",
    "            out_channels = NDF * min(2**(i+1), 8)\n",
    "            stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1\n",
    "            convolved = conv(layers[-1], out_channels, stride=stride)\n",
    "            normalized = batchnorm(convolved)\n",
    "            rectified = lrelu(normalized, 0.2)\n",
    "            layers.append(rectified)\n",
    "\n",
    "    # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\n",
    "    with tf.variable_scope(\"layer_%d\" % (len(layers) + 1)):\n",
    "        convolved = conv(rectified, out_channels=1, stride=1)\n",
    "        output = tf.sigmoid(convolved) # Limits output to (0, 1), a probability\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss & Optimizer\n",
    "\n",
    "The discriminator's loss is simple, just minimize giving the wrong label.\n",
    "\n",
    "For the generator, we have two losses. The first is the standard GAN loss, try to minimize the discriminator detecting that the generated image is fake. The second loss is an L1 loss that tries to minimize the differences between generated and target image, since they should be as similar as possible. L2 loss is also sometimes used here, but the paper notes L1 loss produces less blurring. We then linearly combine these two losses to get our total generator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our generated images\n",
    "with tf.variable_scope(\"generator\"):\n",
    "    outputs = create_generator(inputs)\n",
    "with tf.variable_scope(\"generator\", reuse=True):\n",
    "    tst_outputs = create_generator(tst_inputs)\n",
    "\n",
    "# Discriminator for our \"real\" images\n",
    "with tf.variable_scope(\"discriminator\"):\n",
    "    predict_real = create_discriminator(inputs, targets)\n",
    "\n",
    "# Discriminator for our generated images\n",
    "with tf.variable_scope(\"discriminator\", reuse=True):\n",
    "    predict_fake = create_discriminator(inputs, outputs)\n",
    "\n",
    "# To prevent log(0)\n",
    "EPS = 1e-12\n",
    "\n",
    "# Discriminator Loss\n",
    "discrim_loss = tf.reduce_mean(-(tf.log(predict_real + EPS) + tf.log(1 - predict_fake + EPS)))\n",
    "\n",
    "# Parameters controling how much we weight each loss\n",
    "L1_WEIGHT = 100.\n",
    "GAN_WEIGHT = 1.\n",
    "\n",
    "# Adversarial Loss\n",
    "gen_loss_GAN = tf.reduce_mean(-tf.log(predict_fake + EPS))\n",
    "# L1 Loss\n",
    "gen_loss_L1 = tf.reduce_mean(tf.abs(targets - outputs))\n",
    "# Overall Loss\n",
    "gen_loss = gen_loss_GAN * GAN_WEIGHT + gen_loss_L1 * L1_WEIGHT\n",
    "\n",
    "\n",
    "# Learning Rate\n",
    "LR = 0.0002\n",
    "# Momentum Term\n",
    "BETA1 = 0.5\n",
    "\n",
    "# Discriminator training variables\n",
    "discrim_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"discriminator\")]\n",
    "discrim_optim = tf.train.AdamOptimizer(LR, BETA1)\n",
    "discrim_grads_and_vars = discrim_optim.compute_gradients(discrim_loss, var_list=discrim_tvars)\n",
    "discrim_train = discrim_optim.apply_gradients(discrim_grads_and_vars)\n",
    "\n",
    "# Makes it so discrim must exec first\n",
    "with tf.control_dependencies([discrim_train]):\n",
    "    # All trainable generator variables\n",
    "    gen_tvars = [var for var in tf.trainable_variables() if var.name.startswith(\"generator\")]\n",
    "    gen_optim = tf.train.AdamOptimizer(LR, BETA1)\n",
    "    gen_grads_and_vars = gen_optim.compute_gradients(gen_loss, var_list=gen_tvars)\n",
    "    gen_train = gen_optim.apply_gradients(gen_grads_and_vars)\n",
    "\n",
    "# Maintain moving averages of vars\n",
    "ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "update_losses = ema.apply([discrim_loss, gen_loss_GAN, gen_loss_L1])\n",
    "\n",
    "# Global Step\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "incr_global_step = tf.assign(global_step, global_step+1)\n",
    "\n",
    "\n",
    "train = tf.group(update_losses, incr_global_step, gen_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Training is pretty straightforward, however, one thing to note here is that unlike prior models I've written up this definitely requires a GPU if you're going to do the full 200 epochs, or you'll be waiting a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_images(fetches):\n",
    "    ''' Saves out images from fetches. '''\n",
    "    image_dir = os.path.join('out', \"images\")\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "\n",
    "    filesets = []\n",
    "    for i, in_path in enumerate(fetches[\"paths\"]):\n",
    "        name, _ = os.path.splitext(os.path.basename(in_path.decode(\"utf8\")))\n",
    "        fileset = {\"name\": name}\n",
    "        for kind in [\"inputs\", \"outputs\", \"targets\"]:\n",
    "            filename = name + \"-\" + kind + \".png\"\n",
    "            fileset[kind] = filename\n",
    "            out_path = os.path.join(image_dir, filename)\n",
    "            contents = fetches[kind][i]\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                f.write(contents)\n",
    "        filesets.append(fileset)\n",
    "    return filesets\n",
    "\n",
    "def convert(image):\n",
    "    ''' Converts NN output back to normal image. '''\n",
    "    image = image + 1 / 2 # [-1, 1] => [0, 1]\n",
    "    return tf.image.convert_image_dtype(image, dtype=tf.uint8, saturate=True)\n",
    "\n",
    "# Reverse processing on images so they can be outputted\n",
    "converted_inputs = convert(tst_inputs)\n",
    "converted_targets = convert(tst_targets)\n",
    "converted_outputs = convert(tst_outputs)\n",
    "\n",
    "# Gets image data for inputs, targets, and outputs\n",
    "display_fetches = {\n",
    "    \"paths\": tst_paths,\n",
    "    \"inputs\": tf.map_fn(tf.image.encode_png, converted_inputs, dtype=tf.string, name=\"input_pngs\"),\n",
    "    \"targets\": tf.map_fn(tf.image.encode_png, converted_targets, dtype=tf.string, name=\"target_pngs\"),\n",
    "    \"outputs\": tf.map_fn(tf.image.encode_png, converted_outputs, dtype=tf.string, name=\"output_pngs\"),\n",
    "}\n",
    "\n",
    "MAX_EPOCHS = 200\n",
    "OUTPUT_FREQ = 50\n",
    "SAVE_FREQ = 5000\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "sv = tf.train.Supervisor(logdir='out', saver=None)\n",
    "start = time.time()\n",
    "with sv.managed_session() as sess:\n",
    "    max_steps = steps_per_epoch * MAX_EPOCHS\n",
    "    start = time.time()\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        def should(freq):\n",
    "            ''' Returns true if correct frequency interval. '''\n",
    "            return freq > 0 and ((step + 1) % freq == 0 or step == max_steps - 1)\n",
    "\n",
    "        fetches = {\n",
    "            \"train\": train,\n",
    "            \"global_step\": sv.global_step,\n",
    "        }\n",
    "\n",
    "        if should(OUTPUT_FREQ):\n",
    "            fetches[\"discrim_loss\"] = discrim_loss\n",
    "            fetches[\"gen_loss_GAN\"] = gen_loss_GAN\n",
    "            fetches[\"gen_loss_L1\"] = gen_loss_L1\n",
    "\n",
    "        results = sess.run(fetches)\n",
    "\n",
    "        if should(OUTPUT_FREQ):\n",
    "            train_epoch = math.ceil(results[\"global_step\"] / steps_per_epoch)\n",
    "            train_step = (results[\"global_step\"] - 1) % steps_per_epoch + 1\n",
    "            print(f\"progress  epoch {train_epoch}  step {train_step}\")\n",
    "            print(\"discrim_loss\", results[\"discrim_loss\"])\n",
    "            print(\"gen_loss_GAN\", results[\"gen_loss_GAN\"])\n",
    "            print(\"gen_loss_L1\", results[\"gen_loss_L1\"])\n",
    "\n",
    "        if should(SAVE_FREQ):\n",
    "            print(\"saving model\")\n",
    "            saver.save(sess, os.path.join('out', \"model\"), global_step=sv.global_step)\n",
    "\n",
    "        if sv.should_stop():\n",
    "            break\n",
    "\n",
    "    # Save out images\n",
    "    for step in range(tst_steps_per_epoch):\n",
    "        results = sess.run(display_fetches)\n",
    "        filesets = save_images(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Some results from the facades and maps datasets:\n",
    "\n",
    "| Input | Generated | Target |\n",
    "|-------|-----------|--------|\n",
    "| <img src='imgs/ipynb/pix2pix/results/6-inputs.png'> | <img src='imgs/ipynb/pix2pix/results/6-outputs.png'> | <img src='imgs/ipynb/pix2pix/results/6-targets.png'> |\n",
    "| <img src='imgs/ipynb/pix2pix/results/9-inputs.png'> | <img src='imgs/ipynb/pix2pix/results/9-outputs.png'> | <img src='imgs/ipynb/pix2pix/results/9-targets.png'> |\n",
    "|       |           |        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
