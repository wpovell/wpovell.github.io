{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from [affinelayer's TensorFlow implementation](https://github.com/affinelayer/pix2pix-tensorflow). I would highly recommend checking out [his post](https://github.com/affinelayer/pix2pix-tensorflow) for a high-level overview of how the model works. This post is more meant to walk through the code line-by-line.\n",
    "\n",
    "## Load Data\n",
    "\n",
    "First lets load in our data. To start we're going to work with the facades dataset. The input image will be a labeled version of the second picture of a building facade, with different colors representing different features like windows, doors, etc. Our goal will be to produce the second image from the first.\n",
    "\n",
    "Unlike prior posts, we're going to use some of TensorFlow's utilities for loading in data. We get our list of files, put them into a queue, and have a `WholeFileReader` read and decode each. The format of these images is the first half is the target photo, and the second half is the annotated version.\n",
    "\n",
    "<img src=\"imgs/ipynb/pix2pix/input_example.jpg\">\n",
    "\n",
    "We'll preprocess the image to have pixel values between `[-1, 1]` and then cut it in half, assigning the first part to our target and the second to our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "\n",
    "## Load data ##\n",
    "input_paths = glob.glob('data/pix2pix/facades/train/*.jpg') # All jpgs\n",
    "path_queue = tf.train.string_input_producer(input_paths, shuffle=True) # Produces image paths\n",
    "reader = tf.WholeFileReader()\n",
    "paths, contents = reader.read(path_queue)\n",
    "rawInput = tf.image_decode_jpeg(contents)\n",
    "rawInput = tf.image.convert_image_dtype(rawInput, dtype=tf.float32)\n",
    "\n",
    "# [height, width, channel]\n",
    "rawInput.set_shape([None, None, 3])\n",
    "width = tf.shape(rawInput)[1]\n",
    "\n",
    "preprocess = lambda x: x * 2 - 1 # [0, 1] => [-1, 1]\n",
    "targets = preprocess(rawInput[:,:width//2,:]) # Left side\n",
    "inputs = preprocess(rawInput[:,width//2:,:]) # Right side\n",
    "\n",
    "# TODO: Move\n",
    "BATCH_SIZE = 1\n",
    "paths, inputs, targets = tf.train.batch([paths, inputs, targets], batch_size=BATCH_SIZE)\n",
    "steps_per_epoch = int(math.ceil(len(input_paths) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions\n",
    "\n",
    "Before we start building the model, there are a couple functions we'll be using that we should define.\n",
    "\n",
    "### Conv & Deconv\n",
    "\n",
    "If you're not familiar with Convolution, check out my post on [Convolutional Neural Nets](/posts/cnn-mnist.html). Deconvolution (or Transposed Convolution) was new to me. Normally when performing convolution, we're applying filters in such a way that we decrease the size of our image, effectively downsampling it. Deconvolution does the opposite, upsampling an image to output a larger tensor. There is a lot of debate as what to call this; deconvolution has a well defined meaning from signal processing as the inverse of convolution, which is not what this operation does. Because of this, many people opt to use the name Transposed Convolution instead, which is also what the TensorFlow API does. I'm just going to use `deconv` in code because it's easier to type.\n",
    "\n",
    "What deconvolution ends up looking like is just convolution but with more padding around/between pixels:\n",
    "\n",
    "<div style='text-align: center'>\n",
    "<figure style=\"display: inline-block;\">\n",
    "<img src=\"imgs/ipynb/pix2pix/conv.gif\">\n",
    "<figcaption>Convolution with Stride 2</figcaption>\n",
    "</figure>\n",
    "<figure style=\"display: inline-block;\">\n",
    "<img src=\"imgs/ipynb/pix2pix/deconv.gif\">\n",
    "<figcaption>Deconvolution with Stride 2</figcaption>\n",
    "</figure>\n",
    "<p>Animations from <a href=\"https://github.com/vdumoulin/conv_arithmetic\">here</a></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(batch_input, out_channels, stride):\n",
    "    ''' Convolve input with given stride. '''\n",
    "    in_channels = batch_input.get_shape()[3]\n",
    "    # The trainable filter we create for the conv\n",
    "    conv_filter = tf.get_variable(\"filter\",\n",
    "                                  [4, 4, in_channels, out_channels],\n",
    "                                  dtype=tf.float32,\n",
    "                                  initializer=tf.random_normal_initializer(0, 0.02))\n",
    "\n",
    "    padded_input = tf.pad(batch_input,\n",
    "                          [[0, 0], [1, 1], [1, 1], [0, 0]],\n",
    "                          mode=\"CONSTANT\")\n",
    "    # Output of the conv\n",
    "    conv = tf.nn.conv2d(padded_input,\n",
    "                        conv_filter,\n",
    "                        [1, stride, stride, 1],\n",
    "                        padding=\"VALID\")\n",
    "    return conv\n",
    "\n",
    "def deconv(batch_input, out_channels):\n",
    "    ''' Transposed Convolution. '''\n",
    "    batch, in_height, in_width, in_channels = [int(d) for d in batch_input.get_shape()]\n",
    "\n",
    "    # The trainable filter we create for the deconv\n",
    "    conv_filter = tf.get_variable(\"filter\",\n",
    "                                  [4, 4, out_channels, in_channels],\n",
    "                                  [dtype=tf.float32],\n",
    "                                  initializer=tf.random_normal_initializer(0, 0.02))\n",
    "    # Output of the deconv\n",
    "    conv = tf.nn.conv2d_transpose(batch_input,\n",
    "                                  conv_filter,\n",
    "                                  [batch, in_height * 2, in_width * 2, out_channels],\n",
    "                                  [1, 2, 2, 1],\n",
    "                                  padding=\"SAME\")\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU\n",
    "\n",
    "Leaky ReLU is a variation of normal ReLU that helps prevent dead neurons. With ReLU, values less than 0 are set to 0, and have no gradient. This creates the problem where a neuron can start always outputting 0 for any input and once in that state, can't get out of it since there is no gradient to go up. Leaky ReLU tries to fix this by having values less than zero have a slight negative slope, equivalent to the following:\n",
    "\n",
    "\\begin{align*}\n",
    "\\operatorname{ReLU}(x) &= \\max(0,x) \\\\\n",
    "\\operatorname{LReLU}(x,a) &= \\frac{1+a}{2}x + \\frac{1-a}{2} \\vert x \\vert  \\\\\n",
    "&= \\begin{cases}\n",
    "    x,&  x \\geq 0\\\\\n",
    "    ax,& x < 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img style=\"margin: 5px; border: 1px solid black; display: inline-block; width: 30%\" src=\"imgs/ipynb/pix2pix/relu.png\"><img style=\"margin: 5px; border: 1px solid black; display: inline-block; width: 30%\" src=\"imgs/ipynb/pix2pix/lrelu.png\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, a):\n",
    "    ''' Leaky ReLU. '''\n",
    "    return (0.5 * (1 + a)) * x + (0.5 * (1 - a)) * tf.abs(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Batch Normalization is a cool general technique for improving training. All it does is normalize the input to a layer for mean and variance, while also including a trainable bias and scale parameter that allows the amount of normalization to be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(inp):\n",
    "    ''' Batch Normalization. '''\n",
    "    channels = inp.get_shape()[3]\n",
    "    offset = tf.get_variable(\"offset\", [channels], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "    scale = tf.get_variable(\"scale\", [channels], dtype=tf.float32, initializer=tf.random_normal_initializer(1.0, 0.02))\n",
    "    mean, variance = tf.nn.moments(inp, axes=[0, 1, 2], keep_dims=False)\n",
    "    variance_epsilon = 1e-5\n",
    "    normalized = tf.nn.batch_normalization(inp, mean, variance, offset, scale, variance_epsilon=variance_epsilon)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/ipynb/pix2pix/generator.png\">\n",
    "<img style=\"display:block;width:50%;margin:auto;\" src=\"imgs/ipynb/pix2pix/units.png\">\n",
    "<figcaption>Images from <a href=\"https://affinelayer.com/pix2pix/\">affinelayer</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "### Skip Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of generator filters\n",
    "NGF = 64\n",
    "\n",
    "def create_generator(generator_inputs):\n",
    "    ''' Creates our generator for the given inputs. '''\n",
    "    layers = []\n",
    "\n",
    "    # encoder_1: [batch, 256, 256, 3] => [batch, 128, 128, ngf]\n",
    "    output = conv(generator_inputs, NGF, stride=2)\n",
    "    layers.append(output)\n",
    "\n",
    "    layer_specs = [\n",
    "        NGF * 2, # encoder_2: [batch, 128, 128, ngf] => [batch, 64, 64, ngf * 2]\n",
    "        NGF * 4, # encoder_3: [batch, 64, 64, ngf * 2] => [batch, 32, 32, ngf * 4]\n",
    "        NGF * 8, # encoder_4: [batch, 32, 32, ngf * 4] => [batch, 16, 16, ngf * 8]\n",
    "        NGF * 8, # encoder_5: [batch, 16, 16, ngf * 8] => [batch, 8, 8, ngf * 8]\n",
    "        NGF * 8, # encoder_6: [batch, 8, 8, ngf * 8] => [batch, 4, 4, ngf * 8]\n",
    "        NGF * 8, # encoder_7: [batch, 4, 4, ngf * 8] => [batch, 2, 2, ngf * 8]\n",
    "        NGF * 8, # encoder_8: [batch, 2, 2, ngf * 8] => [batch, 1, 1, ngf * 8]\n",
    "    ]\n",
    "\n",
    "    for out_channels in layer_specs:\n",
    "        rectified = lrelu(layers[-1], 0.2)\n",
    "        # [batch, in_height, in_width, in_channels] => [batch, in_height/2, in_width/2, out_channels]\n",
    "        convolved = conv(rectified, out_channels, stride=2)\n",
    "        output = batchnorm(convolved)\n",
    "        layers.append(output)\n",
    "\n",
    "    layer_specs = [\n",
    "        (NGF * 8, 0.5),   # decoder_8: [batch, 1, 1, ngf * 8] => [batch, 2, 2, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.5),   # decoder_7: [batch, 2, 2, ngf * 8 * 2] => [batch, 4, 4, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.5),   # decoder_6: [batch, 4, 4, ngf * 8 * 2] => [batch, 8, 8, ngf * 8 * 2]\n",
    "        (NGF * 8, 0.0),   # decoder_5: [batch, 8, 8, ngf * 8 * 2] => [batch, 16, 16, ngf * 8 * 2]\n",
    "        (NGF * 4, 0.0),   # decoder_4: [batch, 16, 16, ngf * 8 * 2] => [batch, 32, 32, ngf * 4 * 2]\n",
    "        (NGF * 2, 0.0),   # decoder_3: [batch, 32, 32, ngf * 4 * 2] => [batch, 64, 64, ngf * 2 * 2]\n",
    "        (NGF, 0.0),       # decoder_2: [batch, 64, 64, ngf * 2 * 2] => [batch, 128, 128, ngf * 2]\n",
    "    ]\n",
    "\n",
    "    num_encoder_layers = len(layers)\n",
    "    for decoder_layer, (out_channels, dropout) in enumerate(layer_specs):\n",
    "        skip_layer = num_encoder_layers - decoder_layer - 1\n",
    "        if decoder_layer == 0:\n",
    "            # first decoder layer doesn't have skip connections\n",
    "            # since it is directly connected to the skip_layer\n",
    "            input = layers[-1]\n",
    "        else:\n",
    "            input = tf.concat([layers[-1], layers[skip_layer]], axis=3)\n",
    "\n",
    "        rectified = tf.nn.relu(input)\n",
    "        # [batch, in_height, in_width, in_channels] => [batch, in_height*2, in_width*2, out_channels]\n",
    "        output = deconv(rectified, out_channels)\n",
    "        output = batchnorm(output)\n",
    "\n",
    "        if dropout > 0.0:\n",
    "            output = tf.nn.dropout(output, keep_prob=1 - dropout)\n",
    "\n",
    "        layers.append(output)\n",
    "\n",
    "    # decoder_1: [batch, 128, 128, ngf * 2] => [batch, 256, 256, 3]\n",
    "    inp = tf.concat([layers[-1], layers[0]], axis=3)\n",
    "    rectified = tf.nn.relu(inp)\n",
    "    output = deconv(rectified, 3)\n",
    "    output = tf.tanh(output)\n",
    "    layers.append(output)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriminator\n",
    "\n",
    "<figure>\n",
    "<img src=\"imgs/ipynb/pix2pix/discriminator.png\">\n",
    "<figcaption>Image from <a href=\"https://affinelayer.com/pix2pix/\">affinelayer</a></figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NDF = 64\n",
    "\n",
    "def create_discriminator(discrim_inputs, discrim_targets):\n",
    "    n_layers = 3\n",
    "    layers = []\n",
    "\n",
    "    # 2x [batch, height, width, in_channels] => [batch, height, width, in_channels * 2]\n",
    "    inp = tf.concat([discrim_inputs, discrim_targets], axis=3)\n",
    "\n",
    "    # layer_1: [batch, 256, 256, in_channels * 2] => [batch, 128, 128, ndf]\n",
    "    convolved = conv(inp, NDF, stride=2)\n",
    "    rectified = lrelu(convolved, 0.2)\n",
    "    layers.append(rectified)\n",
    "\n",
    "    # layer_2: [batch, 128, 128, ndf] => [batch, 64, 64, ndf * 2]\n",
    "    # layer_3: [batch, 64, 64, ndf * 2] => [batch, 32, 32, ndf * 4]\n",
    "    # layer_4: [batch, 32, 32, ndf * 4] => [batch, 31, 31, ndf * 8]\n",
    "    for i in range(n_layers):\n",
    "        out_channels = a.ndf * min(2**(i+1), 8)\n",
    "        stride = 1 if i == n_layers - 1 else 2  # last layer here has stride 1\n",
    "        convolved = conv(layers[-1], out_channels, stride=stride)\n",
    "        normalized = batchnorm(convolved)\n",
    "        rectified = lrelu(normalized, 0.2)\n",
    "        layers.append(rectified)\n",
    "\n",
    "    # layer_5: [batch, 31, 31, ndf * 8] => [batch, 30, 30, 1]\n",
    "    convolved = conv(rectified, out_channels=1, stride=1)\n",
    "    output = tf.sigmoid(convolved)\n",
    "    layers.append(output)\n",
    "\n",
    "    return layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
