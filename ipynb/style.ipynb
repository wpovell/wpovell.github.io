{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from [Stanford CS20SI](http://web.stanford.edu/class/cs20si/).\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "\n",
    "def vgg_weights(vgg_layers, layer):\n",
    "    \"\"\" Return the weights and biases already trained by VGG.\n",
    "    This is all related to the format of the VGG model and isn't that interesting.\n",
    "    \"\"\"\n",
    "    W = vgg_layers[0][layer][0][0][2][0][0]\n",
    "    b = vgg_layers[0][layer][0][0][2][0][1]\n",
    "    return W, b.reshape(b.size)\n",
    "\n",
    "def conv2d_relu(vgg_layers, prev_layer, layer):\n",
    "    \"\"\" Return the Conv2D layer with RELU using the weights, biases from the VGG\n",
    "    model at 'layer'.\n",
    "    Inputs:\n",
    "        vgg_layers: holding all the layers of VGGNet\n",
    "        prev_layer: the output tensor from the previous layer\n",
    "        layer: the index to current layer in vgg_layers\n",
    "\n",
    "    Output:\n",
    "        relu applied on the convolution.\n",
    "    \"\"\"\n",
    "    # Get the weights from the vgg model for the current layer\n",
    "    W, b = vgg_weights(vgg_layers, layer)\n",
    "\n",
    "    # These are consts because they're already trained, we won't be changing them\n",
    "    W = tf.constant(W, name='weights')\n",
    "    b = tf.constant(b, name='bias')\n",
    "    \n",
    "    conv2d = tf.nn.conv2d(prev_layer, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv2d + b)\n",
    "\n",
    "def avgpool(prev_layer):\n",
    "    \"\"\" Return the average pooling layer. The paper suggests that average pooling\n",
    "    actually works better than max pooling.\n",
    "    Input:\n",
    "        prev_layer: the output tensor from the previous layer\n",
    "\n",
    "    Output:\n",
    "        the output of the tf.nn.avg_pool() function.\n",
    "    \"\"\"\n",
    "    return tf.nn.avg_pool(prev_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], \n",
    "                          padding='SAME', name='avg_pool_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building The Graph\n",
    "\n",
    "The first thing we do is load in the pre-trained weights from the VGG model. We then create the image variable that we'll be styling. Using the functions defined above, we run the image variable through the layers of convolution and pooling, saving the value at each step.\n",
    "\n",
    "Note: The reason the layer numbers jump around is we skip layers (1,3,4,6, ... etc.) that represent the pooling and ReLU which we are performing ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output image size\n",
    "# Larger images will take longer to train &\n",
    "# may not work as well as VGG was trained on small imgs\n",
    "IMAGE_HEIGHT = 500\n",
    "IMAGE_WIDTH = 666\n",
    "\n",
    "# Use variable instead of placeholder because we're training the intial image to make it\n",
    "# look like both the content image and the style image\n",
    "input_image = tf.Variable(np.zeros([1, IMAGE_HEIGHT, IMAGE_WIDTH, 3]), dtype=tf.float32)\n",
    "\n",
    "# Load in weights from the pre-trained vgg model\n",
    "vgg = scipy.io.loadmat('data/vgg19.mat')\n",
    "vgg_layers = vgg['layers']\n",
    "\n",
    "# Build up our graph, passing the input_image through our trained weights\n",
    "graph = {} \n",
    "graph['conv1_1']  = conv2d_relu(vgg_layers, input_image, 0)\n",
    "graph['conv1_2']  = conv2d_relu(vgg_layers, graph['conv1_1'], 2)\n",
    "graph['avgpool1'] = avgpool(graph['conv1_2'])\n",
    "graph['conv2_1']  = conv2d_relu(vgg_layers, graph['avgpool1'], 5)\n",
    "graph['conv2_2']  = conv2d_relu(vgg_layers, graph['conv2_1'], 7)\n",
    "graph['avgpool2'] = avgpool(graph['conv2_2'])\n",
    "graph['conv3_1']  = conv2d_relu(vgg_layers, graph['avgpool2'], 10)\n",
    "graph['conv3_2']  = conv2d_relu(vgg_layers, graph['conv3_1'], 12)\n",
    "graph['conv3_3']  = conv2d_relu(vgg_layers, graph['conv3_2'], 14)\n",
    "graph['conv3_4']  = conv2d_relu(vgg_layers, graph['conv3_3'], 16)\n",
    "graph['avgpool3'] = avgpool(graph['conv3_4'])\n",
    "graph['conv4_1']  = conv2d_relu(vgg_layers, graph['avgpool3'], 19)\n",
    "graph['conv4_2']  = conv2d_relu(vgg_layers, graph['conv4_1'], 21)\n",
    "graph['conv4_3']  = conv2d_relu(vgg_layers, graph['conv4_2'], 23)\n",
    "graph['conv4_4']  = conv2d_relu(vgg_layers, graph['conv4_3'], 25)\n",
    "graph['avgpool4'] = avgpool(graph['conv4_4'])\n",
    "graph['conv5_1']  = conv2d_relu(vgg_layers, graph['avgpool4'], 28)\n",
    "graph['conv5_2']  = conv2d_relu(vgg_layers, graph['conv5_1'], 30)\n",
    "graph['conv5_3']  = conv2d_relu(vgg_layers, graph['conv5_2'], 32)\n",
    "graph['conv5_4']  = conv2d_relu(vgg_layers, graph['conv5_3'], 34)\n",
    "graph['avgpool5'] = avgpool(graph['conv5_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Images\n",
    "\n",
    "We start by loading in our `STYLE_IMAGE` and `CONTENT_IMAGE`. For the example, I use a picture of Blueno and Starry Night by Picasso:\n",
    "\n",
    "<div style=\"text-align:center; margin-top: 10px;\"><img src=\"imgs/ipynb/blueno.jpg\" style=\"width:40%; display: inline-block;\"></img> <img src=\"imgs/ipynb/starry_night.jpg\" style=\"width:40%; display: inline-block; margin: 0\"></img></div>\n",
    "\n",
    "We crop as necessary and subtract out `MEAN_PIXELS` since the VGG model was trained on mean normalized pixel values.\n",
    "\n",
    "Finally, we generate the slightly noised version of the `CONTENT_IMAGE` that we're going to generate the style of:\n",
    "\n",
    "<img src=\"imgs/ipynb/blueno_noised.png\" style=\"margin: auto; width:40%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "def get_resized_image(img_path, height, width):\n",
    "    image = Image.open(img_path)\n",
    "    image = ImageOps.fit(image, (width, height), Image.ANTIALIAS)\n",
    "    image = np.asarray(image, np.float32)\n",
    "    return np.expand_dims(image, 0)\n",
    "\n",
    "def generate_noise_image(content_image, height, width, noise_ratio=0.6):\n",
    "    \"\"\"Take our content image and fuzz it a bit \n",
    "    \"\"\"\n",
    "    noise_image = np.random.uniform(-20, 20, \n",
    "                                    (1, height, width, 3)).astype(np.float32)\n",
    "    return noise_image * noise_ratio + content_image * (1 - noise_ratio)\n",
    "\n",
    "# parameters to manage experiments\n",
    "STYLE_IMAGE = 'styles/starry_night.jpg'\n",
    "CONTENT_IMAGE = 'content/blueno.jpg'\n",
    "\n",
    "MEAN_PIXELS = np.array([123.68, 116.779, 103.939]).reshape((1,1,1,3))\n",
    "\"\"\" MEAN_PIXELS is defined according to description on their github:\n",
    "https://gist.github.com/ksimonyan/211839e770f7b538e2d8\n",
    "'In the paper, the model is denoted as the configuration D trained with scale jittering. \n",
    "The input images should be zero-centered by mean pixel (rather than mean image) subtraction. \n",
    "Namely, the following BGR values should be subtracted: [103.939, 116.779, 123.68].'\n",
    "\"\"\"\n",
    "\n",
    "content_image = get_resized_image(CONTENT_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "content_image = content_image - MEAN_PIXELS\n",
    "\n",
    "style_image = get_resized_image(STYLE_IMAGE, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "style_image = style_image - MEAN_PIXELS\n",
    "\n",
    "# percentage weight of the noise for intermixing with the content image\n",
    "NOISE_RATIO = 0.6\n",
    "initial_image = generate_noise_image(content_image, IMAGE_HEIGHT, IMAGE_WIDTH, NOISE_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Loss\n",
    "\n",
    "This is where the meat of the algorithm lies. Our goal is to create an image that has the style of one image and the content of another, so we should define our loss with this in mind. One way to express this total loss is as a linear combination of the style loss and content loss. We can then play around with the ratio to find a result that balances the two nicely.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\vec{p} &= \\text{The picture we want the content of} \\\\\n",
    "\\vec{a} &= \\text{The art we want the style of} \\\\\n",
    "\\vec{x} &= \\text{The resulting image we generate} \\\\\n",
    "\\mathcal{L}_{total}(\\vec{p}, \\vec{a}, \\vec{x}) &= \\alpha \\mathcal{L}_{content}(\\vec{p}, \\vec{x}) + \\beta \\mathcal{L}_{style}(\\vec{a}, \\vec{x})\n",
    "\\end{align*}$$\n",
    "\n",
    "### Content Loss\n",
    "\n",
    "The paper defines content loss as follows:\n",
    "\n",
    "Let us define $F^l$ as the activation of layer $l$ in response to our generated image $\\vec{x}$. $F^l_{ij}$ then represents the activation of the $i$th filter at position $j$ in layer $l$. We can similarly define $P^l_{ij}$, but for the original image rather than our generated one.\n",
    "\n",
    "We can then define the loss as the squared error between these values:\n",
    "\n",
    "$$ \\mathcal{L}_{content}(\\vec{p}, \\vec{x}, l) = \\frac{1}{2} \\sum \\limits_{i,j} (F^l_{ij} - P^l_{ij})^2 $$\n",
    "\n",
    "where $l$ is the layer we choose to measure the loss on. For the implementation shown in the notebook, `conv4_2` is chosen.\n",
    "\n",
    "It makes sense that this would be a good measure of content loss; two similar images should activate the CNN similarly. The paper additionally argues for using higher layers of the CNN to measure this loss. Lower layers are very close to the original pixel values, so encode much of the original image (including its style). Higher layers on the other hand are closer to the classification of the image and focus more on structural features, therefore being more a measure of content than style.\n",
    "\n",
    "The paper backs up this reasoning by taking images of random noise and trying to train them so as to minimize the content loss. When the content loss is measured on lower layers, the result is almost pixel-perfect to the original image. Higher layers, however, lose some of the pixel content but are still maintain the overall look of the picture. See the figure at the bottom of this cell for illustration.\n",
    "\n",
    "### Style Loss\n",
    "\n",
    "Now on to style loss. This was definitely the most confusing part of the algorithm. The paper defines style loss as follows:\n",
    "\n",
    "Let us define $F$ the same as we did for content loss. Let us consider $G^l$, the Gram matrix for the set of filters in the layer $l$:\n",
    "\n",
    "$$ G^l_{ij} = \\sum\\limits_k F^l_{ik} F^l_{jk} $$\n",
    "\n",
    "This is esstentially all possible dot products between any two filters of the layer. We then consider $A$ as similarly defined, but for our style image $\\vec{a}$ rather than our generated image $\\vec{x}$. Let us also define $N_l$ as the number of filters a layer $l$ has and $M_l$ to be the size of each one of these filters. The style loss for a particular layer $l$ is then defined as\n",
    "\n",
    "$$ E_l = \\frac{1}{4N^2_l M^2_l} \\sum\\limits_{i,j} (G^l_{ij} - A^l_{ij})^2 $$\n",
    "\n",
    "We then linearly combine the style loss of each layer, each with their own weight $w_l$:\n",
    "\n",
    "$$ \\mathcal{L}_{style}(\\vec{a}, \\vec{x}) = \\sum\\limits_{l=0}^L w_l E_l $$\n",
    "\n",
    "Phew that was confusing. Unlike content loss, I don't have as good of an intuitive understanding of how this works, and based on searches online I'm not alone. Generally, this distance between Gram matrices seems to measure the correlation of features in such a way that isn't spatially dependent and doesn't pick up on a lot of content.\n",
    "\n",
    "The paper argues for this measure of style loss with the figure below. It seems if you try to reconstruct an image from noise by minimizing this loss, you get results that show a similar style but don't have features that correspond spatially to the original image. Additionally, as you use higher layers, you get a result that depicts larger and larger scale style.\n",
    "\n",
    "<img style=\"width: 100%\" src=\"imgs/ipynb/loss_recreation.png\"></imgs>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gram_matrix(F, N, M):\n",
    "    \"\"\" Create and return the gram matrix for tensor F\n",
    "    \"\"\"\n",
    "    F = tf.reshape(F, (M, N))\n",
    "    return tf.matmul(tf.transpose(F), F)\n",
    "\n",
    "def single_style_loss(a, g):\n",
    "    \"\"\" Calculate the style loss at a certain layer\n",
    "    Inputs:\n",
    "        a is the feature representation of the real image\n",
    "        g is the feature representation of the generated image\n",
    "    Output:\n",
    "        the style loss at a certain layer (which is E_l in the paper)\n",
    "    \"\"\"\n",
    "    N = a.shape[3] # number of filters\n",
    "    M = a.shape[1] * a.shape[2] # height times width of the feature map\n",
    "    A = gram_matrix(a, N, M)\n",
    "    G = gram_matrix(g, N, M)\n",
    "    return tf.reduce_sum((G - A) ** 2 / ((2 * N * M) ** 2))\n",
    "\n",
    "# Layers used for style features.\n",
    "STYLE_LAYERS = ['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']\n",
    "W = [0.5, 1.0, 1.5, 3.0, 4.0] # give more weights to deeper layers.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(input_image.assign(style_image))\n",
    "    A = sess.run([graph[layer_name] for layer_name in STYLE_LAYERS])\n",
    "\n",
    "# Get layer losses\n",
    "E = [single_style_loss(A[i], graph[STYLE_LAYERS[i]]) for i in range(len(STYLE_LAYERS))]\n",
    "# Linearly combine layer losses\n",
    "n_layers = len(STYLE_LAYERS)\n",
    "graph['style_loss'] = sum([W[i] * E[i] for i in range(n_layers)])\n",
    "\n",
    "\n",
    "# Layer used for content features.\n",
    "CONTENT_LAYER = 'conv4_2'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(input_image.assign(content_image)) # assign content image to the input variable\n",
    "    p = sess.run(graph[CONTENT_LAYER])\n",
    "f = graph[CONTENT_LAYER]\n",
    "graph['content_loss'] = tf.reduce_sum((f - p) ** 2) / (4.0 * p.size)\n",
    "\n",
    "CONTENT_WEIGHT = 0.01\n",
    "STYLE_WEIGHT = 1\n",
    "graph['total_loss'] = CONTENT_WEIGHT * graph['content_loss'] +\n",
    "                        STYLE_WEIGHT * graph['style_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now on to the fun stuff! This part is pretty straight forward, just initialize the variables and run the optimizer for 300 iterations! It's set up to report loss & time as well as save out the current generated image at various intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "graph['global_step'] = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "LEARNING_RATE = 2.0\n",
    "graph['optimizer'] = tf.train.AdamOptimizer(LEARNING_RATE).minimize(\n",
    "                        graph['total_loss'], \n",
    "                        global_step=graph['global_step'])\n",
    "\n",
    "ITERS = 300\n",
    "skip_step = 1\n",
    "with tf.Session() as sess:\n",
    "    # Initialize vars\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Assign our initial image that we'll be modifying\n",
    "    sess.run(input_image.assign(initial_image))\n",
    "\n",
    "    start_time = time.time()\n",
    "    for index in range(ITERS):\n",
    "        if index >= 5 and index < 20:\n",
    "            skip_step = 10\n",
    "        elif index >= 20:\n",
    "            skip_step = 20\n",
    "\n",
    "        sess.run(graph['optimizer'])\n",
    "        if (index + 1) % skip_step == 0:\n",
    "            gen_image, total_loss = sess.run([input_image, graph['total_loss']])\n",
    "            # Readd the mean we subtracted earlier\n",
    "            gen_image = gen_image + MEAN_PIXELS\n",
    "            print('Step {}\\n   Sum: {:5.1f}'.format(index + 1, np.sum(gen_image)))\n",
    "            print('   Loss: {:5.1f}'.format(total_loss))\n",
    "            print('   Time: {}'.format(time.time() - start_time))\n",
    "            start_time = time.time()\n",
    "\n",
    "            filename = 'outputs/%d.png' % (index)\n",
    "            \n",
    "            image = gen_image[0] # the image\n",
    "            image = np.clip(image, 0, 255).astype('uint8')\n",
    "            scipy.misc.imsave(filename, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "I've only run this on my laptop CPU where 300 iterations takes a couple hours to finish. Often times you won't see much of a change in the image after the first 100 or so iterations, though.\n",
    "\n",
    "<img src=\"imgs/ipynb/blueno.gif\"></img>\n",
    "\n",
    "Some other images I've generated:\n",
    "\n",
    "| Content                            | Style                                | Output                           |\n",
    "|:----------------------------------:|:------------------------------------:|:--------------------------------:|\n",
    "| <img style=\"width:100%\" src=\"imgs/ipynb/blueno.jpg\"> | <img style=\"width:100%\" src=\"imgs/ipynb/picasso2.jpg\"> | <img style=\"width:100%\" src=\"imgs/ipynb/out2.png\"> |\n",
    "|                                    | <img style=\"width:100%\" src=\"imgs/ipynb/hokusai.jpg\">  | <img style=\"width:100%\" src=\"imgs/ipynb/out3.png\"> |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
