{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code taken from [siddk's tensorflow workshop](https://github.com/siddk/tensorflow-workshop)\n",
    "\n",
    "## The Math\n",
    "\n",
    "## Visual Overview of Model\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf \n",
    "\n",
    "# Fetch Datasets from Pickled Files\n",
    "with open(\"data/processed_data/train.pik\", 'r') as f:\n",
    "    trainX, trainY, vocab = pickle.load(f)\n",
    "with open(\"data/processed_data/test.pik\", 'r') as g:\n",
    "    testX, testY, _ = pickle.load(g)\n",
    "\n",
    "# Setup the Model Parameters\n",
    "EMBEDDING_SIZE, LSTM_SIZE, VOCAB_SIZE, WINDOW_SIZE = 30, 256, len(vocab), 20\n",
    "\n",
    "### Start Building the Computation Graph ###\n",
    "\n",
    "# Initializer - initialize our variables from standard normal with stddev 0.1\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "# Setup Placeholders => None argument in shape lets us pass in arbitrary sized batches\n",
    "X = tf.placeholder(tf.int32, shape=[None, WINDOW_SIZE])  \n",
    "Y = tf.placeholder(tf.int32, shape=[None, WINDOW_SIZE])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding Matrix\n",
    "E = tf.get_variable(\"Embedding\", shape=[VOCAB_SIZE, EMBEDDING_SIZE], initializer=initializer)\n",
    "\n",
    "# Embedding Lookup + Dropout\n",
    "embeddings = tf.nn.embedding_lookup(E, X)                        # Shape: [None, WINDOW_SZ, EMBED_SZ]\n",
    "drop_embeddings = tf.nn.dropout(?, ?)\n",
    "\n",
    "# Basic LSTM Cell, Initial State \n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(?)            \n",
    "initial_state = lstm.zero_state(BATCH_SIZE, tf.float32)\n",
    "\n",
    "# Run the LSTM over Inputs to get Outputs, State \n",
    "outputs, final_state = tf.nn.dynamic_rnn(lstm, drop_embeddings,  # Output Shape: [None, WINDOW_SZ, LSTM_SZ]\n",
    "                                         initial_state=initial_state)\n",
    "\n",
    "# Output Layer Variables\n",
    "W_1 = tf.get_variable(\"Output_W\", shape=[?, ?], initializer=initializer)\n",
    "b_1 = tf.get_variable(\"Output_b\", shape=[?], initializer=initializer)\n",
    "\n",
    "# Output Layer Transformation => Outputs is 3D Tensor, so use tensordot\n",
    "output = tf.tensordot(outputs, W_1, axes=[[2],[0]]) + b_1\n",
    "\n",
    "# Compute Loss\n",
    "loss = tf.contrib.seq2seq.sequence_loss(output, Y, tf.ones([BATCH_SIZE, WINDOW_SIZE]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup Optimizer\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "### Launch the Session, to Communicate with Computation Graph ###\n",
    "NUM_EPOCHS, BATCH_SIZE = 1, 50\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables in the graph\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        chunk, state, counter = BATCH_SIZE * WINDOW_SIZE, sess.run(initial_state), 0\n",
    "        for (start, end) in zip(range(0, len(trainX), chunk), range(chunk, len(trainX), chunk)):\n",
    "            batch_x = trainX[start:end].reshape([BATCH_SIZE, WINDOW_SIZE])\n",
    "            batch_y = trainY[start:end].reshape([BATCH_SIZE, WINDOW_SIZE])\n",
    "            curr_cost, state, _ = sess.run([loss, final_state, train_op], \n",
    "                                           feed_dict={X: batch_x, Y: batch_y, initial_state: state, keep_prob: 0.5})\n",
    "            if counter % 100 == 0:\n",
    "                print(\"Step {} Loss: {:.3f}, Perplexity: {:.3f}\".format(counter, curr_cost, np.exp(curr_cost)))\n",
    "            counter +=1\n",
    "    \n",
    "    # Evaluate on Test Data\n",
    "    state, counter, sum_cost = sess.run(initial_state), 0, 0.0\n",
    "    for (start, end) in zip(range(0, len(testX), chunk), range(chunk, len(testX), chunk)):\n",
    "        batch_x = testX[start:end].reshape([BATCH_SIZE, WINDOW_SIZE])\n",
    "        batch_y = testY[start:end].reshape([BATCH_SIZE, WINDOW_SIZE])\n",
    "        cost, state = sess.run([loss, final_state], feed_dict={X: batch_x, Y: batch_y, \n",
    "                                                               initial_state: state, keep_prob: 1.0})\n",
    "        sum_cost, counter = sum_cost + cost, counter + 1\n",
    "    print(\"Test Perplexity: {:.3f}\".format(np.exp(sum_cost / counter)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
