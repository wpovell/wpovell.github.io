{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on [this](http://karpathy.github.io/2016/05/31/rl/) blog post.\n",
    "\n",
    "# Setup\n",
    "\n",
    "Let's first look at how the environment in which our AI will be learning is set up. We'll be using OpenAI's gym module to simulate an opponent. Once we create our environment, we can step our environment with one of three moves, do nothing, move up, and move down. After each move a screenshot of the game is returned; this is what our AI will be basing its actions off of.\n",
    "\n",
    "However, we won't be giving the raw screenshots to our neural net; we'll first do some preprocessing where we remove all the data except the paddles and the ball. Also, we'll actually send the difference between consecutive processed frames since that gives a better sense of movement.\n",
    "\n",
    "| Raw | Processed |\n",
    "|:------------:|:---:|\n",
    "| <img src='imgs/ipynb/pong_raw.png'></img> | <img src='imgs/ipynb/pong_processed.png'></img> |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Make our game\n",
    "env = gym.make(\"Pong-v0\")\n",
    "observation = env.reset()\n",
    "\n",
    "def preprocess(I):\n",
    "    \"\"\" Process 210x160x3 uint8 frame into 6400 (80x80) 1D float vector.\n",
    "    \"\"\"\n",
    "    I = I[35:195]    # Crop\n",
    "    I = I[::2,::2,0] # Downsample by factor of 2\n",
    "    I[I == 144] = 0  # Erase background (background type 1)\n",
    "    I[I == 109] = 0  # Erase background (background type 2)\n",
    "    I[I != 0] = 1    # Everything else (paddles, ball) set to 1\n",
    "    return I.astype(np.float).ravel() # Flatten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model\n",
    "\n",
    "Now that we know what our input looks like, lets build our model. We're going to just use a basic feed forward neural net with a ReLU hidden layer of 200, outputting a probability distribution over the possible moves our AI can make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "IMAGE_SIZE = 80 * 80\n",
    "HIDDEN_SIZE = 200\n",
    "N_ACTIONS = 3\n",
    "\n",
    "# Input image\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None, IMAGE_SIZE],name=\"X\")\n",
    "# Output action\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, N_ACTIONS],name=\"Y\")\n",
    "\n",
    "# Weight vectors\n",
    "W1 = tf.get_variable(\"W1\", [IMAGE_SIZE, HIDDEN_SIZE],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable(\"W2\", [HIDDEN_SIZE, N_ACTIONS],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "hidden = tf.nn.relu(tf.matmul(X, W1))\n",
    "logits = tf.matmul(hidden, W2)\n",
    "prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients\n",
    "\n",
    "So we have our model, but what do we calculate our loss against and how do we train? Unlike a problem such as MNIST classification where you're assigning a digit to an image, whether or not a move was \"good\" or \"bad\" for a given game state is harder to determine. However, the idea behind policy gradients is we do the following:\n",
    "\n",
    "1. While the game is not over:\n",
    "    - Take in a screenshot and output a distribution over actions.\n",
    "    - Sample an action from the distribution and perform it, receiving an immediate reward.\n",
    "        - +1 if AI scores, -1 if opponent scores, 0 otherwise.\n",
    "    - Save the input, action performed, and immediate reward received.\n",
    "2. When the game is over, compute a total reward for each action that is a linear combination of all rewards after it. \n",
    "    - Weight rewards that occur more directly after an action more in its total reward.\n",
    "    - Normalize the mean and variance of the total rewards.\n",
    "3. Encourage or discourage each action taken for its input by using the total reward as a gradient to back propagate through the network.\n",
    "\n",
    "e.g. This very short game\n",
    "\n",
    "| Move | Action | Reward | Total Reward | Total Reward ($\\gamma = 0.5$)* | ~ Normalized Total Rewards |\n",
    "|:-----|:------|:-------|:-----------|:-----------------------------|:--|\n",
    "| 1    | UP     | +1     | $-1 \\times \\gamma^3 + 0 \\times \\gamma^2 + 1 \\times \\gamma + 1$ | +1.375 | +1.112 |\n",
    "| 2    | NOTHING| +1     | $-1 \\times \\gamma^2 + 0 \\times \\gamma + 1$ | +0.75 | +0.542 |\n",
    "| 3    | DOWN   | 0      | $-1 \\times \\gamma + 0$ | -0.5 | -0.599 |\n",
    "| 4    | UP     | -1     | $-1$ | -1 | -1.055 |\n",
    "\n",
    "Note: $\\gamma$ is the parameter we use to control how heavily we weight later rewards. $\\gamma = 1$ would weight all rewards equally, $\\gamma = 0$ would make the total reward equal to only immediate reward.\n",
    "\n",
    "## Loss & Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "GAMMA = 0.99 \n",
    "\n",
    "# Loss between taken action & distribution\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "immediate_rewards = tf.placeholder(dtype=tf.float32, shape=[None,1], name=\"immediate_rewards\")\n",
    "\n",
    "# Function weighting our rewards\n",
    "discount_f = lambda accum, val: accum * GAMMA + val;\n",
    "total_rewards_reversed = tf.scan(discount_f, tf.reverse(immediate_rewards, [True, False]))\n",
    "total_rewards = tf.reverse(total_rewards_reversed, [True, False])\n",
    "\n",
    "# Normalize discounted_epr to standard mean and variance\n",
    "mean, variance = tf.nn.moments(total_rewards, [0], shift=None, name=\"total_reward_moments\")\n",
    "total_rewards -= mean\n",
    "total_rewards /= tf.sqrt(variance + 1e-6)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "# Use normalized_rewards for loss gradient\n",
    "grads = optimizer.compute_gradients(loss, grad_loss=total_rewards)\n",
    "train_op = optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Initialize our variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Last image\n",
    "    prev_x = None\n",
    "\n",
    "    # Histories\n",
    "    x_hist, reward_hist, y_hist = [],[],[]\n",
    "    \n",
    "    # Game vars\n",
    "    running_reward = None\n",
    "    reward_sum = 0\n",
    "    episode_number = 0\n",
    "    \n",
    "    while True:\n",
    "        # Preprocess the observation\n",
    "        cur_x = preprocess(observation)\n",
    "\n",
    "        # Set input to different between current and last image\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(IMAGE_SIZE)\n",
    "        prev_x = cur_x\n",
    "\n",
    "        # Get prob dist for given image\n",
    "        feed = { X: np.reshape(x, (1,-1)) }\n",
    "        sample_prob = sess.run(prob, feed)\n",
    "        sample_prob = sample_prob[0,:]\n",
    "\n",
    "        # Stochastically sample a policy\n",
    "        action = np.random.choice(N_ACTIONS, p=sample_prob)\n",
    "        label = np.zeros_like(sample_prob)\n",
    "        label[action] = 1\n",
    "\n",
    "        # Step the environment\n",
    "        observation, reward, done, info = env.step(action+1)\n",
    "        reward_sum += reward\n",
    "\n",
    "        # Record game history\n",
    "        x_hist.append(x)\n",
    "        y_hist.append(label)\n",
    "        reward_hist.append(reward)\n",
    "\n",
    "        if done:\n",
    "            # Update running reward\n",
    "            if running_reward is None:\n",
    "                running_reward = reward_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.99 + reward_sum * 0.01\n",
    "\n",
    "            # Update parameters\n",
    "            feed = {X: np.vstack(x_hist),\n",
    "                    immediate_rewards: np.vstack(reward_hist),\n",
    "                    Y: np.vstack(y_hist)}\n",
    "            _ = sess.run(train_op,feed)\n",
    "\n",
    "            # Print out progress\n",
    "            if episode_number % 10 == 0:\n",
    "                print('ep {}: reward: {}, mean reward: {:3f}'.format(\n",
    "                    episode_number, reward_sum, running_reward))\n",
    "            else:\n",
    "                print('\\tep {}: reward: {}'.format(episode_number, reward_sum))\n",
    "\n",
    "            # Reset game vars\n",
    "            x_hist, reward_hist, y_hist = [],[],[] # reset game history\n",
    "            observation = env.reset() # reset env\n",
    "            reward_sum = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "One thing to know about this model is it takes a long time to train, simulating the games is slow (at least in OpenAI gym). I trained the model for 5000 episodes, taking about a day on my MacBook Pro. It definitely improved a lot as you can see below, but still did not consistently win against the AI. There are a lot of improvements that could be made to this model, mainly replacing the feed-forward net with a convolutional one.\n",
    "<div style='text-align:center'>\n",
    "<img style='display:inline-block; width: 50%' src='imgs/ipynb/pong_reward.png'></img>\n",
    "</div>\n",
    "\n",
    "<div style='text-align:center'>\n",
    "<figure style=\"display: inline-block; margin: 10px;\">\n",
    "<figcaption>After 100 episodes</figcaption>\n",
    "<video autoplay loop>\n",
    "    <source src=\"imgs/ipynb/bad_pong.mp4\" type=\"video/mp4\" />\n",
    "</video>\n",
    "</figure>\n",
    "\n",
    "<figure style=\"display: inline-block; margin: 10px;\">\n",
    "<figcaption>After 5000 episodes</figcaption>\n",
    "<video autoplay loop>\n",
    "    <source src=\"imgs/ipynb/pong_movie.mp4\" type=\"video/mp4\" />\n",
    "</video>\n",
    "</figure>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
